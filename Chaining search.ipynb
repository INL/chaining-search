{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining search\n",
    "\n",
    "\n",
    "\n",
    "## Sphinx documentatie: https://pythonhosted.org/an_example_pypi_project/sphinx.html\n",
    "## in voorbeelden handige python functies opnemen\n",
    "## zoals ; .sort_values(ascending=False,by=['raw_freq']));  list enz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Search\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:20:16.584326Z",
     "start_time": "2019-02-12T16:20:16.454330Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "\n",
    "def property_freq(df, column_name):\n",
    "    '''\n",
    "    Count values for a certain property in a results DataFrame, and sort them by frequency\n",
    "    Args:\n",
    "        df: DataFrame with results, one row per found token\n",
    "        column_name: Column name (property) to count\n",
    "    Returns:\n",
    "        a DataFrame of the most values for this property, sorted by frequency. Column 'token count' contains the number of tokens, column 'perc' gives the percentage.\n",
    "    '''\n",
    "    df = df.groupby(column_name).size().reset_index(name=\"token count\").sort_values(\"token count\",ascending=False).reset_index(drop=True)\n",
    "    total = df.sum(numeric_only=True, axis=0)\n",
    "    df[\"perc\"] = df[\"token count\"] / total.iloc[0]\n",
    "    return df\n",
    "\n",
    "def df_filter(df_column, regex_or_set):\n",
    "    '''\n",
    "    Helper function to build some condition to filter a Pandas DataFrame, \n",
    "    given a column and some value(s) to filter this column with\n",
    "    \n",
    "    Args:\n",
    "        df_column: a Pandas DataFrame column to filter on\n",
    "        regex_or_set: a regular expression or a list \n",
    "    Returns:\n",
    "        a condition\n",
    "        \n",
    "    >>> words_ending_with_e = df_filter( df_lexicon[\"wordform\"], 'e$' )\n",
    "    >>> df_lexicon_final_e = df_lexicon[ words_ending_with_e ]\n",
    "    '''\n",
    "    if type(regex_or_set) is str and containsRegex(regex_or_set):\n",
    "        filter_condition = df_column.str.contains(regex_or_set)    \n",
    "    elif type(regex_or_set) is list:\n",
    "        filter_condition = df_column.isin(regex_or_set)\n",
    "    else:\n",
    "        found_type = str(type(regex_or_set))\n",
    "        raise ValueError(\"regex_or_set should be a regex or a list (now it has type \"+found_type+\")\")\n",
    "    return filter_condition\n",
    "    \n",
    "    \n",
    "\n",
    "def concat_df(df_arr, keys_arr=None):\n",
    "    '''\n",
    "    This function concatenates two dataframes \n",
    "    Args:\n",
    "        df_arr: array of Pandas DataFrames\n",
    "        keys_arr: array of keys to assign to the records of each DataFrame, so we can still distinguish the original DataFrames\n",
    "    Returns:\n",
    "        a single Pandas DataFrame \n",
    "        \n",
    "    >>> new_df = concat_df( [dataframe1, dataframe2, dataframe3], ['chn corpus', 'nederlab', 'opensonar'] )\n",
    "    >>> display_df(new_df)\n",
    "    '''\n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "    \n",
    "    if keys_arr is not None:\n",
    "        concat_df = pd.concat( df_arr, keys=keys_arr )\n",
    "    else:\n",
    "        concat_df = pd.concat( df_arr )\n",
    "    \n",
    "    return concat_df\n",
    "\n",
    "\n",
    "\n",
    "def join_df(df_arr, join_type=None):\n",
    "    \n",
    "    '''\n",
    "    This function joins two dataframes (=concat along axis 1) \n",
    "    Args:\n",
    "        df_arr: array of Pandas DataFrames\n",
    "        join_type: {inner, outer (default)}\n",
    "    Returns:\n",
    "        a single Pandas DataFrame \n",
    "        \n",
    "    >>> new_df = join_df( [dataframe1, dataframe2] )\n",
    "    >>> display_df(new_df)\n",
    "    '''\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "    \n",
    "    if join_type is None:\n",
    "        concat_df = pd.concat( df_arr, axis=1 )\n",
    "    else:\n",
    "        concat_df = pd.concat( df_arr, axis=1, join=join_type )\n",
    "    \n",
    "    return concat_df\n",
    "    \n",
    "\n",
    "def get_tagger(df_corpus, word_key=\"word\", pos_key=\"universal_dependency\"):\n",
    "    '''\n",
    "    This function instantiates a tagger trained with some corpus annotations \n",
    "    Args:\n",
    "        df_corpus: Pandas DataFrame with annotated corpus data\n",
    "    Returns:\n",
    "        a PerceptronTagger instance \n",
    "    \n",
    "    >>> tagger = get_tagger(df_corpus)  # df_corpus containes a Pandas DataFrame with lots of corpus data\n",
    "    >>> sentence = 'Here is some beautiful sentence'\n",
    "    >>> tagged_sentence = tagger.tag( sentence.split() )\n",
    "    >>> print(tagged_sentence) \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # The corpus DataFrame consists of a number of sentences (rows) with a fixed number of tokens.\n",
    "    # Each token has a fixed number of layers holding info like: lemma, wordform or part-of-speech. \n",
    "    # As a result, the number of columns of each row = [number of tokens] x [number of layers]\n",
    "    \n",
    "    # To be able to feed the tagger correctly, we need to compute the number of layers,\n",
    "    # so we can infer the number of tokens the sentences hold. This is because\n",
    "    # the tagger expects us to feed it with arrays with length = [number of tokens], as elements of\n",
    "    # one single array holding all sentences arrays (see below).\n",
    "    \n",
    "    # So, determine how many layers (lemma, pos, wordform) we have \n",
    "    column_names = list(df_corpus.columns.values)\n",
    "    for n, val in enumerate(column_names):\n",
    "        # remove the numbers at the end of the layers names (lemma 1, lemma 2, ..., pos 1, pos 2, ...)\n",
    "        # so we end up with clean layers name only\n",
    "        column_names[n] = val.split(' ')[0] \n",
    "    number_of_layers = len(set(column_names))\n",
    "\n",
    "    # Now we can determine the standard length of our corpus sentences: that can be computed \n",
    "    # by dividing the number of columns of the corpus DataFrame by the number of layers\n",
    "    # we just computed.\n",
    "    sentences = []\n",
    "    nr_of_words_per_sentence = int( df_corpus.shape[1] / number_of_layers )  \n",
    "\n",
    "    # Build training data for the tagger in the right format\n",
    "    # The input must be like: [ [('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')], [...] ]\n",
    "    for index, row in df_corpus.iterrows():\n",
    "        one_sentence =  []\n",
    "        wrong = False\n",
    "        for i in range(0, nr_of_words_per_sentence, 1):\n",
    "            word_idx = word_key+' '+str(i)\n",
    "            pos_idx = pos_key+' '+str(i)\n",
    "            tuple = ( row[word_idx], row[pos_idx] )\n",
    "            one_sentence.append( tuple )\n",
    "            if (row[word_idx] is None or row[pos_idx] is None):\n",
    "                wrong = True\n",
    "        if wrong is False:\n",
    "            sentences.append(one_sentence)\n",
    "\n",
    "    # Instantiate and train the tagger now\n",
    "    tagger = PerceptronTagger(load=False)\n",
    "    tagger.train(sentences)\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:20:18.282678Z",
     "start_time": "2019-02-12T16:20:18.218463Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import urllib\n",
    "#import wx   # for interaction popups          TODO -> omzetten naar JS of zo\n",
    "import itertools # for frequency list function and from_iterable\n",
    "import numpy     # idem\n",
    "from IPython.display import FileLink, FileLinks\n",
    "AVAILABLE_CORPORA = {'chn':'http://svprmc05.inl.nl/blacklab-server/chn',\n",
    "                     'opensonar':'http://172.16.10.93:8080/blacklab-server/opensonar',\n",
    "                     'zeebrieven':'http://svprmc20.ivdnt.org/blacklab-server/zeebrieven',\n",
    "                     'gysseling':'http://svprmc20.ivdnt.org/blacklab-server/gysseling',\n",
    "                     'nederlab':''}\n",
    "AVAILABLE_LEXICA = ['anw', 'celex', 'diamant', 'duelme', 'molex']\n",
    "RECORDS_PER_PAGE = 1000\n",
    "\n",
    "# Fields parsed by default from corpus xml by _parse_xml\n",
    "# Extra fields can be given to _parse_xml by users\n",
    "DEFAULT_FIELDS_TOKEN = [\"word\", \"lemma\", \"universal_dependency\"]\n",
    "DEFAULT_FIELDS_DOC = []\n",
    "\n",
    "# Get rid of ellipsis in display (otherwise relevant data might not be shown)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "\n",
    "\n",
    "# Search methods\n",
    "\n",
    "def search_corpus_allwords(corpus, pos=None):\n",
    "    '''\n",
    "    This function gets all words of a corpus. If needed, the output can be restricted to words with a given part-of-speech\n",
    "    Args:\n",
    "        corpus: corpus name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus_allwords(\"gysseling\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    query = r'[word=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[word=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus)\n",
    "\n",
    "\n",
    "def search_corpus_alllemmata(corpus, pos):\n",
    "    '''\n",
    "    This function gets all lemmata of a corpus. If needed, the output can be restricted to lemmata with a given part-of-speech\n",
    "    Args:\n",
    "        corpus: corpus name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus_alllemmata(\"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    query = r'[lemma=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[lemma=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus) \n",
    "\n",
    "\n",
    "def corpus_options(detailed_context=False, extra_fields_doc=[], extra_fields_token=[], start_position=1):\n",
    "    '''\n",
    "    Helper function to declare options to be applied in a corpus search.\n",
    "    This function is to be called as a parameter of search_corpus()\n",
    "    \n",
    "    >>> corpus_query = r'[lemma=\"someword\"]'\n",
    "    >>> corpus_opts = corpus_options( extra_fields_doc=fields[\"document\"] )\n",
    "    >>> df_corpus = search_corpus( corpus_query, corpus_to_search, corpus_opts )\n",
    "    '''\n",
    "    return {\n",
    "        \"detailed_context\": detailed_context, \n",
    "        \"extra_fields_doc\": extra_fields_doc,\n",
    "        \"extra_fields_token\": extra_fields_token,\n",
    "        \"start_position\": start_position\n",
    "    }\n",
    "\n",
    "def search_corpus(query, corpus, corpus_options=None):\n",
    "    '''\n",
    "    This function searches a corpus given a query and a corpus name\n",
    "    Args:\n",
    "        query: a corpus query, eg. previously generated by corpus_query_lemma() or such\n",
    "        corpus: a corpus name\n",
    "        detailed_context: (optional) {True, False (default)} \n",
    "        extra_fields_doc: \n",
    "        extra_fields_token: \n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus(r'[pos=\"ADJ\"][word=\"huis\"]', \"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # read options\n",
    "    detailed_context = False if corpus_options is None else corpus_options[\"detailed_context\"]\n",
    "    extra_fields_doc = [] if corpus_options is None else corpus_options[\"extra_fields_doc\"]\n",
    "    extra_fields_token = [] if corpus_options is None else corpus_options[\"extra_fields_token\"]\n",
    "    start_position = 1 if corpus_options is None else corpus_options[\"start_position\"]\n",
    "    \n",
    "    # show wait indicator\n",
    "    show_wait_indicator('Searching '+corpus+ ' at page '+str(start_position))    \n",
    "    \n",
    "    if corpus not in AVAILABLE_CORPORA:\n",
    "        raise ValueError(\"Unknown corpus: \" + corpus)\n",
    "    try:\n",
    "        # Do request to federated content search corpora, so we get same output format for every corpus\n",
    "        url = \"http://portal.clarin.inl.nl/fcscorpora/clariah-fcs-endpoints/sru?operation=searchRetrieve&queryType=fcs&maximumRecords=1000&startRecord=\" + str(start_position) + \"&x-fcs-context=\" + corpus + \"&query=\" + urllib.parse.quote(query)\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        response_text = response.text    \n",
    "        df, next_page = _parse_xml(response_text, detailed_context, extra_fields_doc, extra_fields_token)\n",
    "        # If there are next pages, call search_corpus recursively\n",
    "        #print(next_page)\n",
    "        remove_wait_indicator()\n",
    "        if next_page > 0:\n",
    "            df_more = search_corpus(query, corpus, corpus_options)\n",
    "            df = df.append(df_more, ignore_index=True)\n",
    "            \n",
    "        \n",
    "        # show message out of xml, if some error has occured (prevents empty output)\n",
    "        _show_error_if_any(response_text)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        remove_wait_indicator()\n",
    "        raise ValueError(\"An error occured when searching corpus \" + corpus + \": \"+ str(e))\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "def search_corpus_multiple(queries, corpus):\n",
    "    '''\n",
    "    This function sends multiples queries at once to the search_corpus function\n",
    "    Args:\n",
    "        queries: array of corpus queries, eg. previously generated by corpus_query_lemma() or such\n",
    "        corpus: a corpus name \n",
    "    Returns:\n",
    "        a dictionary of Pandas DataFrames, associating each query (key) to the resulting corpus data (value)\n",
    "    '''\n",
    "    result_dict = {}\n",
    "    for query in queries:\n",
    "        result_dict[query] = search_corpus(query,corpus)\n",
    "    return result_dict\n",
    "   \n",
    "    \n",
    "\n",
    "def search_lexicon_alllemmata(lexicon, pos=None):\n",
    "    '''\n",
    "    This function gets all lemmata of a lexicon. If needed, the output can be restricted to lemmata with a given part-of-speech\n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing lexicon data \n",
    "        \n",
    "    >>> df_corpus = search_corpus_alllemmata(\"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    query = _lexicon_query_alllemmata(lexicon, pos)\n",
    "    return search_lexicon(query, lexicon)\n",
    "\n",
    "\n",
    "\n",
    "def search_lexicon(query, lexicon):\n",
    "    '''\n",
    "    This function searches a lexicon given a query and a lexicon name\n",
    "    Args:\n",
    "        query: a lexicon query, typically previously generated by lexicon_query() or such \n",
    "        lexicon: a lexicon name\n",
    "    Returns:\n",
    "        a Pandas DataFrame with lexicon data \n",
    "        \n",
    "    '''\n",
    "     # show wait indicator, so the user knows what's happening\n",
    "    show_wait_indicator('Searching '+lexicon)\n",
    "    \n",
    "    # default endpoint, except when diamant is invoked\n",
    "    endpoint = \"http://172.16.4.56:8890/sparql\"\n",
    "    if (lexicon==\"diamant\"):\n",
    "        endpoint = \"http://svprre02:8080/fuseki/tdb/sparql\"\n",
    "    \n",
    "    try:\n",
    "        # Accept header is needed for virtuoso, it isn't otherwise!\n",
    "        response = requests.post(endpoint, data={\"query\":query}, headers = {\"Accept\":\"application/sparql-results+json\"})\n",
    "        \n",
    "        response_json = json.loads(response.text)\n",
    "        records_json = response_json[\"results\"][\"bindings\"]\n",
    "        records_string = json.dumps(records_json)    \n",
    "        df = pd.read_json(records_string, orient=\"records\")\n",
    "    \n",
    "        # make sure cells containing NULL are added too, otherwise we'll end up with ill-formed data\n",
    "        # CAUSES MALFUNCTION: df = df.fillna('')\n",
    "        df = df.applymap(lambda x: '' if pd.isnull(x) else x[\"value\"])         \n",
    "        \n",
    "        # remove wait indicator, \n",
    "        remove_wait_indicator()\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        remove_wait_indicator()\n",
    "        raise ValueError(\"An error occured when searching lexicon \" + lexicon + \": \"+ str(e))          \n",
    "        \n",
    "\n",
    "# Processing methods\n",
    "\n",
    "def column_difference(df_column1, df_column2):\n",
    "    '''\n",
    "    This function computes differences and similarities between two Pandas DataFrames\n",
    "    Args:\n",
    "        df_column1: a Pandas DataFrame, filtered by one column\n",
    "        df_column2: a Pandas DataFrame, filtered by one column\n",
    "    Returns:\n",
    "        diff_left: array of words only in df_column1\n",
    "        diff_right: array of words only in df_column2\n",
    "        intersec: array of words both in df_column1 and df_column2\n",
    "        \n",
    "    >>> diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "    >>> display( 'These words are only in DataFrame #1 : ' + \", \".join(diff_left) )\n",
    "    >>> display( 'These words are only in DataFrame #2 : ' + \", \".join(diff_right) )\n",
    "    >>> display( 'These words are common to both DataFrame : ' + \", \".join(intersec) )\n",
    "    '''\n",
    "    \n",
    "    set_df1 = set(df_column1)\n",
    "    set_df2 = set(df_column2)\n",
    "    diff_left = set_df1.difference(set_df2)\n",
    "    diff_right = set_df2.difference(set_df1)\n",
    "    intersec = set_df1.intersection(set_df2)\n",
    "    return diff_left, diff_right, intersec\n",
    "\n",
    "\n",
    "def diamant_get_synonyms(df):\n",
    "    '''\n",
    "    This function gets lemmata or definitions out of a Pandas DataFrame with Diamant data. \n",
    "    The output set content depends on the result type.\n",
    "    \n",
    "    Args:\n",
    "        df: a Pandas DataFrame containing Diamant data\n",
    "    Returns:\n",
    "        a set of lemmata OR a set of synonym definitions\n",
    "        \n",
    "    >>> query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "    >>> df_lexicon = search_lexicon(query, lexicon)\n",
    "    >>> syns = diamant_get_synonyms(df_lexicon) \n",
    "    >>> display( 'Synoniemen voor ' + search_word + ': ' + \", \".join(syns)))\n",
    "    '''\n",
    "    \n",
    "    # Depending on the result type, we return the lemma or the definition text\n",
    "    lemmas = set(df[df[\"inputMode\"]==\"defText\"][\"n_ontolex_writtenRep\"])\n",
    "    defTexts = set(df[df[\"inputMode\"]==\"lemma\"][\"n_syndef_definitionText\"])\n",
    "    return lemmas|defTexts\n",
    "\n",
    "\n",
    "def _parse_xml(text, detailed_context=False, extra_fields_doc=[], extra_fields_token=[]):\n",
    "    '''\n",
    "    This function converts the XML output of a lexicon or corpus search into a Pandas DataFrame for further processing\n",
    "    \n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "        detailed_context: (optional) True to parse the layers of all tokens, False to limit detailed parsing to hits\n",
    "        extra_fields_doc: \n",
    "        extra_fields_token: \n",
    "    Returns:\n",
    "        df: a Pandas DataFrame representing the parse results\n",
    "        next_pos: the next result page to be parsed (since the results might be spread among several XML response pages), \n",
    "        or 0 if there is no page left to be parsed\n",
    "    '''\n",
    "    \n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    records = []\n",
    "    records_len = []\n",
    "    n_tokens = 0\n",
    "    computed_nt = False\n",
    "    cols= []\n",
    "    \n",
    "    fields_token = DEFAULT_FIELDS_TOKEN + extra_fields_token\n",
    "    fields_doc = DEFAULT_FIELDS_DOC + extra_fields_doc\n",
    "    for entry in root.iter(\"{http://clarin.eu/fcs/resource}ResourceFragment\"):\n",
    "        doc_metadata = {}\n",
    "        for dataView in entry.findall(\"{http://clarin.eu/fcs/resource}DataView\"):\n",
    "            # Parse document metadata\n",
    "            if(dataView.get(\"type\")==\"application/x-clariah-fcs-simple-metadata+xml\"):\n",
    "                for keyval in dataView.findall(\"keyval\"):\n",
    "                    key = keyval.get(\"key\")\n",
    "                    if key in fields_doc:\n",
    "                        value = keyval.get(\"value\")\n",
    "                        doc_metadata[key] = value\n",
    "            \n",
    "            # ----- [part 1] ----- \n",
    "            # in 'hits only' mode, we'll gather the hits, otherwise we'll gather all the words of the sentences\n",
    "            \n",
    "            # We only take hits into account, ignore metadata and segmenting dataViews\n",
    "            if (detailed_context is False and dataView.get(\"type\")==\"application/x-clarin-fcs-hits+xml\"):\n",
    "                result = dataView.find(\"{http://clarin.eu/fcs/dataview/hits}Result\")\n",
    "                left_context = result.text if result.text is not None else ''\n",
    "                hits = list(result)\n",
    "                if len(hits)==0:\n",
    "                    print([w for w in result.itertext()])\n",
    "                    print(\"no hit in kwic, skip\")\n",
    "                    continue\n",
    "                last_hit = hits[-1]\n",
    "                right_context = last_hit.tail if last_hit.tail is not None else ''\n",
    "                #hit_words = [hit.text for hit in hits]\n",
    "            \n",
    "            # ----- [part 2] ----- \n",
    "            # gather info about each hit (=hits only mode) or about each word (=NOT hits only mode)\n",
    "            \n",
    "            # Get lemma of each hit\n",
    "            cols= []\n",
    "            max_len = 0\n",
    "            if (dataView.get(\"type\")==\"application/x-clarin-fcs-adv+xml\"):\n",
    "                hit_layer = defaultdict(list) \n",
    "                for layer in dataView.findall(\".//{http://clarin.eu/fcs/dataview/advanced}Layer\"):\n",
    "                    layer_id = layer.get(\"id\").split(\"/\")[-1]\n",
    "                    # Only capture this layer, if it is in the list of designated fields (default+extra by user)\n",
    "                    if layer_id in fields_token:\n",
    "                        path = \".//{http://clarin.eu/fcs/dataview/advanced}Span\"\n",
    "                        if (detailed_context is False):\n",
    "                            path = path+\"[@highlight='h1']\" \n",
    "                        for one_span in layer.findall(path):\n",
    "                            span_text = one_span.text            \n",
    "                            hit_layer[layer_id].append(span_text)\n",
    "                        # Compute number of columns\n",
    "                        n_tokens = len(hit_layer[layer_id])\n",
    "                        if max_len<n_tokens:\n",
    "                            max_len = n_tokens\n",
    "                data, cols = _combine_layers(hit_layer, n_tokens, doc_metadata_req=fields_doc, doc_metadata_recv=doc_metadata)\n",
    "                if detailed_context is False:\n",
    "                    kwic = [left_context] + data + [right_context]\n",
    "                else:\n",
    "                    kwic = data\n",
    "                records.append(kwic)\n",
    "                records_len.append(n_tokens)\n",
    "                \n",
    "    if detailed_context is False:\n",
    "        columns = [\"left context\"] + cols + [\"right context\"]\n",
    "    else:\n",
    "        columns = cols\n",
    "    \n",
    "    next_pos = 0\n",
    "    next_record_position = root.find(\"{http://docs.oasis-open.org/ns/search-ws/sruResponse}nextRecordPosition\")\n",
    "    if (next_record_position is not None):\n",
    "        next_pos = int(next_record_position.text)\n",
    "        \n",
    "        \n",
    "    # do some clean up now!\n",
    "    for i in range( len(records_len), 0, 1): \n",
    "        if (records_len[i]<max_len):\n",
    "            del records[i]\n",
    "        \n",
    "    return pd.DataFrame(records, columns = columns), next_pos\n",
    "\n",
    "\n",
    "def _combine_layers(hit_layer, n_tokens, doc_metadata_req, doc_metadata_recv):\n",
    "    '''\n",
    "    Combine the layers, in alphabetical order of the layer names, to a flat list, with separate column per layer per word in hit, and document metadata added as last columns\n",
    "    \n",
    "    Args:\n",
    "        hit_layer: dictionary with list of items per layer\n",
    "        n_tokens: number of tokens for which token-level annotations exist.\n",
    "                    Is equal to total number of tokens in sentence if _parse_xml is called with detailed_context=True.\n",
    "                    Is equal to number of tokens in hit if _parse_xml is called with detailed_context=False.\n",
    "        doc_metadata_req: list of document metadata fields which have been requested\n",
    "        doc_metadata_recv: dictionary with document metadata that is actually present in hits:\n",
    "                        can contain less fields than doc_fields_requested\n",
    "    Returns:\n",
    "        data: flat list with combined token layers, sorted alphabetically, and document metadata\n",
    "    '''\n",
    "    # Sort layer keys to ensure same order of data in every row and column titles\n",
    "    layers_keys = sorted(hit_layer.keys())\n",
    "    # Original structure is list of tokens per layer id\n",
    "    # Arrange items first on token, then on layer_id\n",
    "    layers_token_flat = [hit_layer[layer_id][n] for n in range(n_tokens) for layer_id in layers_keys]\n",
    "    # Flatten list of document metadata fields\n",
    "    # Use all requested fields, some of which may not be available in this hit\n",
    "    doc_flat = [doc_metadata_recv[field] if field in doc_metadata_recv else \"\" for field in doc_metadata_req]\n",
    "    # Combine token and document data\n",
    "    data = layers_token_flat + doc_flat\n",
    "    \n",
    "    ### Columns\n",
    "    # Create list of columns, in same order\n",
    "    tokens_columns = [layer_id+ \" \"+str(n) for n in range(n_tokens) for layer_id in layers_keys]\n",
    "    # Add all requested document metadata fields as columns\n",
    "    columns = tokens_columns + doc_metadata_req\n",
    "    return data, columns\n",
    "\n",
    "def _show_error_if_any(text):\n",
    "    '''\n",
    "    This function reads error messages in the XML output of a lexicon or corpus search \n",
    "    and it finds any, it is printed on screen\n",
    "    \n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    root = ET.fromstring(text)\n",
    "    msgs = []\n",
    "    for diagnostic in root.iter(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}diagnostic\"):\n",
    "        for msg in diagnostic.findall(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}message\"):\n",
    "            msg_text = msg.text if msg.text is not None else ''\n",
    "            msgs.append(msg_text)\n",
    "    if len(msgs) > 0:\n",
    "        print(\"; \".join(msgs))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def get_frequency_list(lexicon, pos, corpus):\n",
    "    '''\n",
    "    This function builds a lemmata frequency list of a corpus, \n",
    "    given a lexicon (for obvious reasons limited to some part-of-speech).\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: a part-of-speech to limit the search to\n",
    "        corpus: the corpus to be searched\n",
    "    Returns:\n",
    "        a Pandas DataFrame with raw frequencies ('raw_freq' column) and rankings ('rank' column)\n",
    "        \n",
    "    >>> df_frequency_list = get_frequency_list(some_lexicon, \"NOUN\", corpus_to_search)\n",
    "    >>> display(df_frequency_list)\n",
    "    '''\n",
    "    \n",
    "    print('Beware: building a frequency list can take a long time')\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "\n",
    "    # instantiate a dataframe for storing lemmata and frequencies\n",
    "    df_frequency_list = pd.DataFrame(index=lexicon_lemmata_arr, columns=['raw_freq'])\n",
    "    df_frequency_list.index.name = 'lemmata'\n",
    "\n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, and count the results\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "\n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "\n",
    "        # store frequencies\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                raw_freq = len(df_corpus[df_corpus['lemma 0'] == one_lemma])\n",
    "                df_frequency_list.at[one_lemma, 'raw_freq'] = raw_freq \n",
    "                \n",
    "    # final step: compute rank\n",
    "    # this is needed to be able to compare different frequency lists \n",
    "    # with each other (which we could achieve by computing a rank diff)\n",
    "    df_frequency_list['rank'] = df_frequency_list['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    return df_frequency_list;\n",
    "\n",
    "\n",
    "def get_missing_wordforms(lexicon, pos, corpus):    \n",
    "    '''\n",
    "    This function gathers all paradigms of a lexicon with a given part-of-speech\n",
    "    and searches an annotated corpus for words missing in those paradigms\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: a part-of-speech to limit the search to\n",
    "        corpus: the corpus to be searched\n",
    "    Returns:\n",
    "        a Pandas DataFrame associating lemmata to their paradigms ('known_wordforms' column) and\n",
    "        missing wordforms found in the corpus ('unknown_wordforms' column).\n",
    "        \n",
    "    >>> df = get_missing_wordforms(\"molex\", \"VERB\", \"opensonar\")\n",
    "    >>> df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "    '''\n",
    "    \n",
    "    print('Beware: finding missing wordforms in a lexicon can take a long time');\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "    \n",
    "    # instantiate a dataframe for storing lemmata and wordforms\n",
    "    df_enriched_lexicon = pd.DataFrame(index=lexicon_lemmata_arr, columns=['lemma', 'pos', 'known_wordforms', 'unknown_wordforms'])\n",
    "    df_enriched_lexicon.index.name = 'lemmata'\n",
    "    \n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, \n",
    "    # and compute difference between both\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "        \n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "        \n",
    "        # process results\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                \n",
    "                # look up the known wordforms in the lexicon\n",
    "                query = lexicon_query(one_lemma, pos, lexicon)\n",
    "                df_known_wordforms = search_lexicon(query, lexicon)\n",
    "                \n",
    "                if (len(df_known_wordforms) != 0):\n",
    "                    known_wordforms = set( df_known_wordforms['wordform'].str.lower() )\n",
    "                    # find the wordforms in the corpus\n",
    "                    corpus_wordforms = set( (df_corpus[df_corpus['lemma 0'] == one_lemma])['word 0'].str.lower() )\n",
    "                    # determine which corpus wordforms are not in lexicon wordforms\n",
    "                    unknown_wordforms = corpus_wordforms.difference(known_wordforms)\n",
    "\n",
    "                    if (len(unknown_wordforms) !=0):\n",
    "                        # store the results\n",
    "                        df_enriched_lexicon.at[one_lemma, 'lemma'] = one_lemma\n",
    "                        df_enriched_lexicon.at[one_lemma, 'pos'] = pos\n",
    "                        df_enriched_lexicon.at[one_lemma, 'known_wordforms'] = known_wordforms\n",
    "                        df_enriched_lexicon.at[one_lemma, 'unknown_wordforms'] = unknown_wordforms\n",
    "                \n",
    "    # return non-empty results, t.i. cases in which we found some wordforms\n",
    "    return df_enriched_lexicon[ df_enriched_lexicon['unknown_wordforms'].notnull() ]\n",
    "        \n",
    "    \n",
    "def get_rank_diff(df1, df2):\n",
    "    '''\n",
    "    This function compares the rankings of words common to two dataframes, and compute a rank_diff, in such\n",
    "    a way that one can see which words are very frequent in one set and rare in the other.\n",
    "    \n",
    "    Args:\n",
    "        df1: a Pandas DataFrame\n",
    "        df2: a Pandas DataFrame\n",
    "    Returns:\n",
    "        a Pandas DataFrame with lemmata (index), ranks of both input dataframes ('rank_1' and 'rank_2' columns) \n",
    "        and the rank_diff ('rank_diff' column).\n",
    "        \n",
    "    >>> df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "    >>> df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "    >>> df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "    '''\n",
    "    \n",
    "    # Find lemmata shared by both dataframes: computing ranks diffs is only possible\n",
    "    # when dealing with lemmata which are in both frames\n",
    "    lemmata_list1 = set(df1.index.tolist())\n",
    "    lemmata_list2 = set(df2.index.tolist())\n",
    "    common_lemmata_list = list( lemmata_list1.intersection(lemmata_list2) )\n",
    "    \n",
    "    # Build dataframes limited to the common lemmata\n",
    "    limited_df1 = df1.loc[ common_lemmata_list , : ]\n",
    "    limited_df2 = df2.loc[ common_lemmata_list , : ]\n",
    "    \n",
    "    # Recompute ranks in both dataframes, because in each frame the original ranks were\n",
    "    # computed with a lemmata list which might be larger than the lemmata list common\n",
    "    # to both dataframes\n",
    "    \n",
    "    limited_df1['rank'] = limited_df1['raw_freq'].rank(ascending = False).astype(int)\n",
    "    limited_df2['rank'] = limited_df2['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    # Instantiate a dataframe for storing lemmata and rank diffs\n",
    "    df_rankdiffs = pd.DataFrame(index=common_lemmata_list, columns=['rank_1', 'rank_2', 'rank_diff'])\n",
    "    df_rankdiffs.index.name = 'lemmata'\n",
    "    \n",
    "    df_rankdiffs['rank_1'] = limited_df1['rank']\n",
    "    df_rankdiffs['rank_2'] = limited_df2['rank']\n",
    "    df_rankdiffs['rank_diff'] = pd.DataFrame.abs( df_rankdiffs['rank_1'] - df_rankdiffs['rank_2'] )\n",
    "    \n",
    "    return df_rankdiffs\n",
    "\n",
    "\n",
    "# TODO: Method misses token fields which are extracted from POS tag by FCS (eg. inflection)\n",
    "def _parse_blacklab_metadata(text):\n",
    "    '''\n",
    "    This method parses metadata fields from a Blacklab metadata response\n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "    Returns:\n",
    "        A dictionary of with lists of document and token metadata\n",
    "    '''\n",
    "    \n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    doc_fields = [md.get(\"name\") for md in root.iter(\"metadataField\")]\n",
    "    token_fields = [prop.get(\"name\") for prop in root.iter(\"property\")]\n",
    "    return {\"document\": doc_fields, \"token\": token_fields}\n",
    "    \n",
    "\n",
    "def _corpus_metadata_blacklab(corpus_name):\n",
    "    '''\n",
    "    Return all possible metadata fields for a BlackLab-based corpus, by sending a request to the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus_name: Name of the corpus\n",
    "    Returns:\n",
    "        A dictionary of with lists of document and token metadata\n",
    "    '''\n",
    "    corpus_url = AVAILABLE_CORPORA[corpus_name]\n",
    "    response = requests.get(corpus_url)\n",
    "    response_text = response.text  \n",
    "    return _parse_blacklab_metadata(response_text)\n",
    "\n",
    "def get_available_metadata(resource_name, resource_type=None):\n",
    "    '''\n",
    "    Return all possible metadata fields for a lexicon or corpus\n",
    "    \n",
    "    Args:\n",
    "        resource_name: Name of the lexicon or corpus\n",
    "        resource_type: (optional) One of 'lexicon' or 'corpus'. Can be used to disambiguate when resource name can be both a lexicon or corpus\n",
    "    Returns:\n",
    "        A list of metadata fields\n",
    "    '''\n",
    "    \n",
    "    # Infer resource type from name\n",
    "    if resource_name in AVAILABLE_CORPORA and resource_name not in AVAILABLE_LEXICA:\n",
    "        res_type = \"corpus\"\n",
    "    elif resource_name in AVAILABLE_LEXICA and resource_name not in AVAILABLE_CORPORA:\n",
    "        res_type = \"lexicon\"\n",
    "    elif resource_name in AVAILABLE_LEXICA and resource_name in AVAILABLE_CORPORA:\n",
    "        if resource_type is not None:\n",
    "            res_type = resource_type\n",
    "        else:\n",
    "            raise ValueError(\"Resource \" + resource_name + \" can be a corpus or lexicon. Please specify the resource_type.\")\n",
    "    else:\n",
    "        raise ValueError(\"Resource \" + resource_name + \" not found.\")\n",
    "    \n",
    "    \n",
    "    if res_type==\"lexicon\":\n",
    "        # Create sample query for this lexicon\n",
    "        q = lexicon_query(word=\"\", pos=\"\", lexicon=resource_name)\n",
    "        return _metadata_from_lexicon_query(q)\n",
    "    elif res_type==\"corpus\":\n",
    "        if resource_name in AVAILABLE_CORPORA and resource_name != \"nederlab\":\n",
    "            return _corpus_metadata_blacklab(resource_name)\n",
    "        elif corpus_name==\"nederlab\":\n",
    "            print(\"Corpus metadata not yet available for Nederlab\")\n",
    "            return []\n",
    "    else:\n",
    "        raise ValueError(\"resource_type should be 'corpus' or 'lexicon'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:20:19.558443Z",
     "start_time": "2019-02-12T16:20:19.529539Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "from IPython.display import Javascript\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "DEFAULT_QUERY = r'[lemma=\"boek\" & pos=\"verb\"]' #r'[lemma=\"boeken\" pos=\"verb\"]'\n",
    "DEFAULT_CORPUS = \"chn\"\n",
    "\n",
    "\n",
    "def show_wait_indicator(message=None):\n",
    "    \n",
    "    print('...' + (message if message else 'Busy now') + '...', end=\"\\r\") \n",
    "    sys.stdout.write(\"\\033[F\")\n",
    "\n",
    "def remove_wait_indicator():    \n",
    "    print('                                                                    ', end=\"\\r\")\n",
    "    sys.stdout.write(\"\\033[F\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def create_corpus_ui():\n",
    "    '''\n",
    "    This function builds a GUI for corpus search\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    # Create UI elements\n",
    "    corpusQueryField = widgets.Text(description=\"<b>CQL query:</b>\", value=DEFAULT_QUERY)\n",
    "    corpusField = widgets.Dropdown(\n",
    "        options=AVAILABLE_CORPORA.keys(),\n",
    "        value=DEFAULT_CORPUS,\n",
    "        description='<b>Corpus:</b>',\n",
    "    )\n",
    "    '''corpusSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    corpusSearchButton.on_click(corpus_search)'''\n",
    "    \n",
    "    # Stack UI elements in vertical box and display\n",
    "    corpusUiBox = widgets.VBox([corpusQueryField,corpusField])\n",
    "    display(corpusUiBox)\n",
    "    \n",
    "    # Return fields, so their contents are accessible from the global namespace of the Notebook\n",
    "    return corpusQueryField, corpusField\n",
    "\n",
    "def create_lexicon_ui():\n",
    "    '''\n",
    "    This function builds a GUI for lexicon search.\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    DEFAULT_SEARCHWORD = 'boek'\n",
    "    DEFAULT_LEXICON = \"diamant\"\n",
    "\n",
    "    # Create UI elements\n",
    "    searchWordField = widgets.Text(description=\"<b>Word:</b>\", value=DEFAULT_SEARCHWORD)\n",
    "    lexiconField = widgets.Dropdown(\n",
    "        options=AVAILABLE_LEXICA,\n",
    "        value=DEFAULT_LEXICON,\n",
    "        description='<b>Lexicon:</b>',\n",
    "    )\n",
    "    '''lexSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    lexSearchButton.on_click(lexicon_search)'''\n",
    "    # Stack UI elements in vertical box and display\n",
    "    lexUiBox = widgets.VBox([searchWordField,lexiconField])\n",
    "    display(lexUiBox)\n",
    "    return searchWordField, lexiconField\n",
    "\n",
    "\n",
    "def create_save_dataframe_ui(df, filename=None):\n",
    "    '''\n",
    "    This function builds a GUI for saving the results of some lexicon or corpus query to a .csv file.\n",
    "    One can use load_dataframe(filepath) to reload the results later on.\n",
    "    \n",
    "    Args:\n",
    "        df: a Pandas DataFrame\n",
    "        filename (Optional): a filename\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    # build ui for saving results\n",
    "    DEFAULT_FILENAME = 'mijn_resultaten.csv' if filename is None else re.sub('[\\W_]+', '', filename)+\".csv\"\n",
    "    saveResultsCaption = widgets.Label(value='Sla uw resultaten op:')\n",
    "    fileNameField = widgets.Text(value=DEFAULT_FILENAME)\n",
    "    savebutton = widgets.Button(\n",
    "        description='Bestand opslaan',\n",
    "        disabled=False,\n",
    "        button_style='warning', \n",
    "        tooltip=DEFAULT_FILENAME,  # trick to pass filename to button widget\n",
    "        icon=''\n",
    "    )\n",
    "    # inject dataframe into button object\n",
    "    savebutton.df = df\n",
    "    # when the user types a new filename, it will be passed to the button tooltip property straight away\n",
    "    fileNameLink = widgets.jslink((fileNameField, 'value'), (savebutton, 'tooltip'))\n",
    "    # click event with callback\n",
    "    savebutton.on_click( _save_dataframe )    \n",
    "    saveResultsBox = widgets.HBox([saveResultsCaption, fileNameField, savebutton])\n",
    "    display(saveResultsBox)\n",
    "    \n",
    "def _save_dataframe(button):\n",
    "    fileName = button.tooltip\n",
    "    # The result files can be saved locally or on the server:\n",
    "    # If result files are to be offered as downloads, set to True; otherwise set to False    \n",
    "    fileDownloadable = False\n",
    "    # specify paths here, if needed:\n",
    "    filePath_onServer = ''  # could be /path/to\n",
    "    filePath_default = ''\n",
    "    # compute full path given chosen mode\n",
    "    fullFileName = (filePath_onServer if fileDownloadable else filePath_default ) + fileName\n",
    "        \n",
    "    try:\n",
    "        button.df.to_csv( fullFileName, index=False)\n",
    "        # confirm it all went well\n",
    "        print(fileName + \" saved\")    \n",
    "        button.button_style = 'success'\n",
    "        button.icon = 'check'\n",
    "        # trick: https://stackoverflow.com/questions/31893930/download-csv-from-an-ipython-notebook\n",
    "        if (fileDownloadable):\n",
    "            downloadableFiles = FileLinks(filePath_onServer)\n",
    "            display(downloadableFiles)\n",
    "    except Exception as e:\n",
    "        button.button_style = 'danger'\n",
    "        raise ValueError(\"An error occured when saving \" + fileName + \": \"+ str(e))    \n",
    "\n",
    "    \n",
    "    \n",
    "def load_dataframe(filepath):\n",
    "    '''\n",
    "    This functions (re)loads some previously saved Pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the saved Pandas DataFrame (.csv)\n",
    "    Returns: \n",
    "        a Pandas DataFrame representing the content of the file\n",
    "    \n",
    "    >>> df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "    >>> display_df(df_corpus, labels=\"Results:\")\n",
    "    '''\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(filepath + \" loaded successfully\")            \n",
    "    except Exception as e:\n",
    "        raise ValueError(\"An error occured when loading \" + filepath + \": \"+ str(e))\n",
    "    finally:\n",
    "        return df\n",
    "    \n",
    "    \n",
    "def display_df(dfs, labels=None, mode='table'):\n",
    "    '''\n",
    "    This function shows the content of one or more Pandas DataFrames.\n",
    "    When dealing with more DataFrames, those should be part of a dictionary associating\n",
    "    labels (eg. corpus or lexicon names) to DataFrames (values). \n",
    "    \n",
    "    Args:\n",
    "        results: a Pandas DataFrames, or a dictionary of Pandas DataFrames\n",
    "        labels: a label, of a list of labels corresponding to the Pandas DataFrames in the first parameter\n",
    "        mode (Optional): Way of displaying, one of 'table' (default) or 'chart' \n",
    "    Returns:\n",
    "        N/A\n",
    "        \n",
    "    >>> result_dict = search_corpus_multiple(queries, corpus)\n",
    "    >>> display_df(result_dict, labels=list(syns))\n",
    "    '''\n",
    "    if type(dfs) is dict:\n",
    "    \n",
    "        assert len(labels)==len(dfs)\n",
    "        for n,query in enumerate(dfs):\n",
    "            df = dfs[query]\n",
    "            if not df.empty:\n",
    "                _display_single_df(df, labels[n], mode)\n",
    "    else:\n",
    "        _display_single_df(dfs, labels, mode)\n",
    "\n",
    "\n",
    "def _display_single_df(df_column, label, mode):\n",
    "    \n",
    "    # chart mode\n",
    "    if mode == 'chart':\n",
    "        plt.figure()\n",
    "        df_column.plot.barh().set_title(label)\n",
    "    \n",
    "    # table mode (default)\n",
    "    else:    \n",
    "        if label is not None:\n",
    "            display(HTML(\"<b>%s</b>\" % label))        \n",
    "\n",
    "        display(df_column)\n",
    "    \n",
    "    # eventually, give UI to save data\n",
    "    create_save_dataframe_ui(df_column, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:20:22.145552Z",
     "start_time": "2019-02-12T16:20:22.115251Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def containsRegex(word):\n",
    "    '''\n",
    "    This function checks whether some string contains a regular expression or not\n",
    "    \n",
    "    Args:\n",
    "        word: a string to check for regular expressions\n",
    "    Returns:\n",
    "        A boolean\n",
    "    '''\n",
    "    return ( word.find('^')>-1 or\n",
    "            word.find('$')>-1 or \n",
    "            re.match(\"\\(.+?\\)\", word) or\n",
    "            re.match(\"\\[.+?\\]\", word) or\n",
    "            re.match(\"[\\+*]\", word) )\n",
    "                     \n",
    "def lexicon_query(word, pos, lexicon):\n",
    "    '''\n",
    "    This function builds a query for getting the paradigm etc. of a given lemma out of a given lexicon.\n",
    "    The resulting query string is to be used as a parameter of search_lexicon() \n",
    "    \n",
    "    Args:\n",
    "        word: a lemma/wordform to build the query with\n",
    "        pos: a part-of-speech to build the query with\n",
    "        lexicon: a lexicon to build the query for\n",
    "    Returns:\n",
    "        a query string to be used as a parameter of search_lexicon() \n",
    "    '''\n",
    "    \n",
    "    if (lexicon==\"anw\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?definition, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "              subpart =  \"\"\"\n",
    "                { { ?lemId rdfs:label ?lemma .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                UNION\n",
    "                { ?definitionId lemon:value ?definition .\n",
    "                values ?definition { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } } .\n",
    "                \"\"\"               \n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  PREFIX anwsch: <http://rdf.ivdnt.org/schema/anw/>\n",
    "                  PREFIX lemon: <http://lemon-model.net/lemon#>\n",
    "                  \n",
    "                  SELECT ?lemId ?lemma ?writtenForm ?definition concat('', ?definitionComplement) as ?definitionComplement\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:sense ?senseId .\n",
    "                      ?senseId lemon:definition ?definitionId .\n",
    "                      ?definitionId lemon:value ?definition .\n",
    "                      OPTIONAL { ?definitionId anwsch:definitionComplement ?definitionComplement .}\n",
    "                      OPTIONAL { ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                          ?lemCFId ontolex:writtenRepresentation ?writtenForm . }\n",
    "                      \"\"\"+subpart+\"\"\"\n",
    "                      }\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        subpart2 = \"\"\"?n_syndef diamant:definitionText ?n_syndef_definitionText .  \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart1 =  \"\"\"\n",
    "                { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "                values ?n_ontolex_writtenRep { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"                \n",
    "            subpart2 = \"\"\"\n",
    "                { ?n_syndef diamant:definitionText ?n_syndef_definitionText . \n",
    "                values ?n_syndef_definitionText { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select ?n_entry ?n_form ?n_ontolex_writtenRep ?n_syndef ?n_sensedef ?n_sensedef_definitionText ?n_syndef_definitionText ?n_sense ?inputMode ?wy_f_show ?wy_t_show\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            \"\"\" + subpart1 + \"\"\"\n",
    "            { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_syndef diamant:definitionText ?n_syndef_definitionText } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "              ?n_sense diamant:attestation ?n_attest_show .\n",
    "              ?n_sense diamant:attestation ?n_attest_filter .\n",
    "              ?n_attest_show diamant:text ?n_q_show .\n",
    "              ?n_attest_filter diamant:text ?n_q_filter .\n",
    "              ?n_attest_show a diamant:Attestation .\n",
    "              ?n_attest_filter a diamant:Attestation .\n",
    "              ?n_q_filter a diamant:Quotation .\n",
    "              ?n_q_show a diamant:Quotation .\n",
    "              ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "              ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "              ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "              ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "              FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "              FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "            { bind(\"lemma\" as ?inputMode) } .\n",
    "            } UNION\n",
    "          {\n",
    "            \"\"\" + subpart2 + \"\"\"\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep } .  { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            ?n_sense diamant:attestation ?n_attest_show .\n",
    "            ?n_sense diamant:attestation ?n_attest_filter .\n",
    "            ?n_attest_filter diamant:text ?n_q_filter .\n",
    "            ?n_attest_show diamant:text ?n_q_show .\n",
    "            ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "            ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "            ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "            ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "            ?n_attest_show a diamant:Attestation .\n",
    "            ?n_attest_filter a diamant:Attestation .\n",
    "            ?n_q_filter a diamant:Quotation .\n",
    "            ?n_q_show a diamant:Quotation .\n",
    "            FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "            FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "          { bind(\"defText\" as ?inputMode) } .\n",
    "            }\n",
    "        }\n",
    "        }\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"\"\"\"\n",
    "        subpart2 = \"\"\"\"\"\"\n",
    "        subpartPos = \"\"\"\"\"\"\n",
    "        if (word != ''):\n",
    "            if (exactsearch == True):\n",
    "                subpart1 =  \"\"\"\n",
    "                    { ?lemCFId ontolex:writtenRep ?lemma . \n",
    "                    values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                    UNION\n",
    "                    { ?wordformId ontolex:writtenRep ?wordform . \n",
    "                    values ?wordform { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } .\n",
    "                    \"\"\"        \n",
    "            else:\n",
    "                subpart2 = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (pos is not None and pos != ''):\n",
    "            subpartPos = \"\"\"FILTER ( regex(?lemPos, \\\"\"\"\"+pos+\"\"\"$\\\") ) .\"\"\"\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://universaldependencies.org/u/>\n",
    "            PREFIX diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "            \n",
    "            SELECT ?lemEntryId ?lemma ?lemPos ?wordformId ?wordform ?hyphenation ?wordformPos ?Gender ?Number\n",
    "            FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "            WHERE\n",
    "            {\n",
    "            ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "            ?lemCFId ontolex:writtenRep ?lemma .\n",
    "            \"\"\"+subpart1+\"\"\"\n",
    "            OPTIONAL {?lemEntryId UD:Gender ?Gender .}\n",
    "            OPTIONAL {?lemEntryId UD:VerbForm ?verbform .}\n",
    "            ?lemEntryId UD:pos ?lemPos .\n",
    "            \"\"\"+subpartPos+\"\"\"\n",
    "            ?lemEntryId ontolex:lexicalForm ?wordformId .\n",
    "            ?wordformId UD:pos ?wordformPos .\n",
    "            OPTIONAL {?wordformId UD:Number ?Number .}\n",
    "            OPTIONAL {?wordformId ontolex:writtenRep ?wordform .}\n",
    "            OPTIONAL {?wordformId diamant:hyphenation ?hyphenation .}\n",
    "            \"\"\"+subpart2+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) .\"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?y lmf:hasLemma ?dl .  \n",
    "                values ?dl { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX duelme: <http://rdf.ivdnt.org/lexica/duelme>\n",
    "            PREFIX intskos: <http://ivdnt.org/schema/lexica#>\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            \n",
    "            SELECT ?exampleSentence ?lemma ?gender ?number\n",
    "            WHERE  {\n",
    "                  ?d intskos:ExampleSentence ?exampleSentence .\n",
    "                  ?d lmf:ListOfComponents [lmf:Component ?y] .\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "                  OPTIONAL {?y UD:Gender ?gender}\n",
    "                  OPTIONAL {?y UD:Number ?number}\n",
    "            \"\"\"+subpart+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX celex: <http://rdf.ivdnt.org/lexica/celex>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            PREFIX decomp: <http://www.w3.org/ns/lemon/decomp#>\n",
    "            PREFIX gold: <http://purl.org/linguistics/gold#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemmaId ?lemma ?wordformId ?wordform ?number ?gender concat('', ?subLemmata) AS ?subLemmata\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .\n",
    "                \"\"\"+subpart+\"\"\"\n",
    "                BIND( ?lemmaId AS ?lemmaIdIRI ).\n",
    "                ?lemmaId ontolex:lexicalForm ?wordformId .\n",
    "                ?wordformId ontolex:writtenRep ?wordform .\n",
    "                OPTIONAL {?wordformId UD:Number ?number} .\n",
    "                OPTIONAL {\n",
    "                    ?lemmaId UD:Gender ?g . \n",
    "                        bind( \n",
    "                            if(?g = UD:Fem_Gender, \n",
    "                            UD:Com_Gender, \n",
    "                                if(?g = UD:Masc_Gender,\n",
    "                                    UD:Com_Gender,\n",
    "                                    UD:Neut_Gender\n",
    "                                )\n",
    "                            )\n",
    "                            AS ?gender\n",
    "                        )\n",
    "                }\n",
    "                OPTIONAL {\n",
    "                    SELECT ?lemmaIdIRI (group_concat(DISTINCT concat(?partNr,\":\",?subLemma);separator=\" + \") as ?subLemmata)\n",
    "                    WHERE {\n",
    "                        SELECT ?lemmaIdIRI ?celexComp ?aWordformId ?subLemma ?partNr\n",
    "                        WHERE {\n",
    "                                {\n",
    "                                ?lemmaIdIRI ontolex:lexicalForm ?aWordformId . \n",
    "                                ?lemmaIdIRI decomp:constituent ?celexComp .\n",
    "                                OPTIONAL { ?celexComp gold:stem [ontolex:writtenRep ?subLemma] . }\n",
    "                                OPTIONAL { ?celexComp decomp:correspondsTo [ ontolex:canonicalForm [ontolex:writtenRep ?subLemma]] . }\n",
    "                                }\n",
    "                                {\n",
    "                                    {\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_4> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_5> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_6> ?celexComp .}                                        \n",
    "                                    }\n",
    "                                ?lemmaIdIRI ?rdfsynt ?celexComp .\n",
    "                                BIND(IF(STRSTARTS(str(?rdfsynt), \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"), replace(STRAFTER(str(?rdfsynt), \"#\"), \"_\", \"\"), \"999\") AS ?partNr) .\n",
    "                                MINUS {\n",
    "                                    ?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#0> ?celexComp .\n",
    "                                    }\n",
    "                                }\n",
    "                            FILTER (?partNr != \"999\") .\n",
    "                            }\n",
    "                            ORDER BY ?partNr\n",
    "                            }\n",
    "                        GROUP BY ?aWordformId ?lemmaIdIRI\n",
    "                    }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def corpus_query_lemma(lemma):\n",
    "    '''\n",
    "    This function builds a query for getting occurances of a given lemma within a given corpus\n",
    "    Args:\n",
    "        lemma: a lemma to look for \n",
    "    Returns:\n",
    "        a corpus query string\n",
    "        \n",
    "    >>> lemma_query = corpus_query_lemma(\"lopen\")\n",
    "    >>> df_corpus = search_corpus(lemma_query, \"chn\")\n",
    "    >>> display(df_corpus)\n",
    "    '''\n",
    "    return r'[lemma=\"'+ lemma + r'\"]'\n",
    "\n",
    "\n",
    "def corpus_query_wordform(word):\n",
    "    '''\n",
    "    This function builds a query for getting occurances of a given wordform within a given corpus\n",
    "    Args:\n",
    "        word: a wordform to look for \n",
    "    Returns:\n",
    "        a corpus query string\n",
    "        \n",
    "    >>> wordform_query = corpus_query_wordform(\"liep\")\n",
    "    >>> df_corpus = search_corpus(wordform_query, \"chn\")\n",
    "    >>> display(df_corpus)\n",
    "    '''\n",
    "    return r'[word=\"'+ word + r'\"]'\n",
    "\n",
    "def _lexicon_query_alllemmata(lexicon, pos):\n",
    "    '''\n",
    "    This function builds a query for getting all lemmata of a lexicon, if needed restricted to a given part-of-speech.\n",
    "    The resulting query string is to be used as a parameter of search_lexicon().\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name \n",
    "        pos: (optional) a part-of-speech\n",
    "    Returns:\n",
    "        a lexicon query string\n",
    "    '''\n",
    "    \n",
    "    if (lexicon==\"anw\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>                  \n",
    "                  SELECT DISTINCT ?writtenForm\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                      ?lemCFId ontolex:writtenRepresentation ?writtenForm .\n",
    "                      }\n",
    "                      ORDER BY ?writtenForm\"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .                \n",
    "                }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select DISTINCT ?n_ontolex_writtenRep AS ?writtenForm\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "        ORDER BY ?n_ontolex_writtenRep\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "            }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        # part-of-speech filter\n",
    "        pos_condition = \"\"\"\"\"\"\n",
    "        if pos is not None and pos != '':\n",
    "            pos_condition = \"\"\"\n",
    "            {?lemEntryId UD:pos ?lemPos .\n",
    "            FILTER regex(?lemPos, '\"\"\"+pos+\"\"\"') } .\n",
    "            \"\"\"\n",
    "        query = \"\"\"\n",
    "                PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                PREFIX UD: <http://universaldependencies.org/u/>\n",
    "                SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "                FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "                WHERE\n",
    "                {\n",
    "                ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "                ?lemCFId ontolex:writtenRep ?lemma .  \n",
    "                \"\"\"+pos_condition+\"\"\"\n",
    "                }\n",
    "                 ORDER BY ?lemma\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Lexicon \" + lexicon + \" not supported for querying all words.\")\n",
    "        \n",
    "    #print(query)\n",
    "    return query\n",
    "\n",
    "def _metadata_from_lexicon_query(lex_query):\n",
    "    '''\n",
    "    Extract metadata fields from a lexicon query string\n",
    "    \n",
    "    Args:\n",
    "        lex_query: A query string issued to a lexicon, can be constructed using lexicon_query()\n",
    "    Returns:\n",
    "        A list of metadata fields\n",
    "    '''\n",
    "    # Get part after select, eg: \"?x ?y ?concat('',z) as ?a\"\n",
    "    select_match = re.search(r'select\\s+(?:distinct)*\\s*(.*)\\s*(?:where|from)', lex_query, flags=re.IGNORECASE)\n",
    "    if select_match:\n",
    "        select_string = select_match.group(1)\n",
    "        #Delete concat() part and following AS, because it can contain a space we do not want to split on\n",
    "        string_wh_concat = re.sub(r'concat\\(.*\\) AS', '', select_string, flags=re.IGNORECASE)\n",
    "        split_string = string_wh_concat.split()\n",
    "        for i,elem in enumerate(split_string):\n",
    "            if elem.lower()==\"AS\":\n",
    "                # Remove AS and element before AS\n",
    "                split_string.pop(i)\n",
    "                split_string.pop(i-1)\n",
    "                # Assume only one AS, so we escape loop\n",
    "                break\n",
    "        columns = [c.lstrip(\"?\") for c in split_string]\n",
    "    else:\n",
    "        raise ValueError(\"No columns find in lexicon query.\")\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:33:10.242933Z",
     "start_time": "2019-02-12T13:33:10.226983Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:33:12.240805Z",
     "start_time": "2019-02-12T13:33:11.769658Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus = corpusField.value\n",
    "df_corpus = search_corpus(query, corpus)\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, labels=\"Results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:33:40.926998Z",
     "start_time": "2019-02-12T13:33:40.908161Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:33:43.042638Z",
     "start_time": "2019-02-12T13:33:42.640296Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import queries, search\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "display(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 1 (parallel): Frequency of *puur*+verb and *zuiver*+verb compared\n",
    "* Below cell searches for *puur*+verb and for *zuiver*+verb in the CHN corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:33:49.963176Z",
     "start_time": "2019-02-12T13:33:49.035825Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"puur\"\n",
    "df_corpus1 = search_corpus('[word=\"' + word1 + r'\"][pos=\"verb\"]',corpus=\"chn\")\n",
    "display(HTML('<b>' + word1 + '</b>'))\n",
    "display(df_corpus1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"zuiver\"\n",
    "df_corpus2 = search_corpus(r'[word=\"' + word2 + r'\"][pos=\"verb\"]',\"chn\")\n",
    "display(HTML('<b>' + word2 + '</b>'))\n",
    "display(df_corpus2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2 (sequential): Retrieve synonyms from DiaMaNT, look up in Gysseling\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T13:34:04.808628Z",
     "start_time": "2019-02-12T13:33:53.939014Z"
    }
   },
   "outputs": [],
   "source": [
    "search_word = \"boek\"\n",
    "lexicon = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "syns = diamant_get_synonyms(df_lexicon) \n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query_lemma(syn) for syn in syns]\n",
    "## Search for all synonyms in corpus\n",
    "result_dict = search_corpus_multiple(syns_queries, corpus)\n",
    "display_df(result_dict, labels=list(syns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:24:19.655999Z",
     "start_time": "2019-01-11T16:24:19.645252Z"
    }
   },
   "source": [
    "## Case study (parallel) 3: Find corpus words not in lexicon; list most frequent ones.\n",
    "* Only parallel if you can ask the lexicon a list of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:20:29.860732Z",
     "start_time": "2019-02-12T16:20:28.664297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Searching anw...\r",
      "\u001b[F                                                                    \r",
      "\u001b[F"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['heemstedenaar',\n",
       " 'heemsteedse',\n",
       " 'heen-en-weerkind',\n",
       " 'heenmatch',\n",
       " 'heenwedstrijd',\n",
       " 'heerdenaar',\n",
       " 'heerdense',\n",
       " 'heerdese',\n",
       " 'heerenveense',\n",
       " 'heerenvener',\n",
       " 'heerlenaar',\n",
       " 'heerlense',\n",
       " 'heerlijk',\n",
       " 'heethoofd',\n",
       " 'heffingsperiode',\n",
       " 'heidesafari',\n",
       " 'heideschaap',\n",
       " 'heilbot',\n",
       " 'heiligendag',\n",
       " 'heiloose',\n",
       " 'heiloor',\n",
       " 'heimwee',\n",
       " 'heisessie',\n",
       " 'heistenaar',\n",
       " 'heistse',\n",
       " 'heldenaar',\n",
       " 'heldendood',\n",
       " 'heldense',\n",
       " 'heldentenor',\n",
       " 'heldergroen',\n",
       " 'helgroen',\n",
       " 'heli',\n",
       " 'helicon',\n",
       " 'heligate',\n",
       " 'helihaven',\n",
       " 'helikopter',\n",
       " 'helikoptergeld',\n",
       " 'helikopterouder',\n",
       " 'helikoptertaxi',\n",
       " 'heliodruk',\n",
       " 'heliogravure',\n",
       " 'helitaxi',\n",
       " 'helix',\n",
       " 'hellehond',\n",
       " 'helm',\n",
       " 'helmdraad',\n",
       " 'helmonder',\n",
       " 'helmondse',\n",
       " 'helper',\n",
       " 'helperssyndroom',\n",
       " 'helpie',\n",
       " 'helpster',\n",
       " 'hematologie',\n",
       " 'hematologisch',\n",
       " 'hematoloog',\n",
       " 'hemd',\n",
       " 'hemelvaartsdag',\n",
       " 'hemelwaarts',\n",
       " 'hen',\n",
       " 'henegouwer',\n",
       " 'henegouwse',\n",
       " 'hengel',\n",
       " 'hengelaar',\n",
       " 'hengelclub',\n",
       " 'hengelose',\n",
       " 'hengelor',\n",
       " 'hengelsportvereniging',\n",
       " 'hengelsportwinkel',\n",
       " 'hengelsportwinkelier',\n",
       " 'hengst',\n",
       " 'hennepteelt',\n",
       " 'heraut',\n",
       " 'herbarium',\n",
       " 'herberg',\n",
       " 'herbivoor',\n",
       " 'herdenkingscollage',\n",
       " 'herdenkingsdag',\n",
       " 'herder',\n",
       " 'herdershond',\n",
       " 'heremietkreeft',\n",
       " 'herenboer',\n",
       " 'herenclub',\n",
       " 'herenhuis',\n",
       " 'herenkleding',\n",
       " 'herfst',\n",
       " 'herfstachtig',\n",
       " 'herfstavond',\n",
       " 'herfstblad',\n",
       " 'herfstbos',\n",
       " 'herfstcollectie',\n",
       " 'herfstdag',\n",
       " 'herfstdraad',\n",
       " 'herfstig',\n",
       " 'herfstkleur',\n",
       " 'herfstkleurig',\n",
       " 'herfstlandschap',\n",
       " 'herfstlucht',\n",
       " 'herfstmaand',\n",
       " 'herfstmanoeuvre',\n",
       " 'herfstmiddag',\n",
       " 'herfstmorgen',\n",
       " 'herfstnacht',\n",
       " 'herfstnachtevening',\n",
       " 'herfstnevel',\n",
       " 'herfstnummer',\n",
       " 'herfstochtend',\n",
       " 'herfstseizoen',\n",
       " 'herfststorm',\n",
       " 'herfsttij',\n",
       " 'herfsttijd',\n",
       " 'herfsttint',\n",
       " 'herfsttrek',\n",
       " 'herfstvakantie',\n",
       " 'herfstweer',\n",
       " 'herfstwind',\n",
       " 'herfstzon',\n",
       " 'herhalingsblessure',\n",
       " 'herheri',\n",
       " 'heriheri',\n",
       " 'herinneringskunst',\n",
       " 'herkeuringsraad',\n",
       " 'herlevingstermijn',\n",
       " 'hermafrodiet',\n",
       " 'hermelijn',\n",
       " 'hermelijnkoorts',\n",
       " 'hermelijnvlinder',\n",
       " 'hermelijnvlo',\n",
       " 'herniablessure',\n",
       " 'heropbloei',\n",
       " 'hersengadget',\n",
       " 'hersenspoeling',\n",
       " 'hersteller',\n",
       " 'herstelperiode',\n",
       " 'herstelrecht',\n",
       " 'hersteltijd',\n",
       " 'hersteltrainer',\n",
       " 'hert',\n",
       " 'hertenkalf',\n",
       " 'hertenleren',\n",
       " 'hertsleren',\n",
       " 'hes',\n",
       " 'hesje',\n",
       " 'hesp',\n",
       " 'het-woord',\n",
       " 'heterofiel',\n",
       " 'heterohuwelijk',\n",
       " 'heupairbag',\n",
       " 'heupblessure',\n",
       " 'heupbroek',\n",
       " 'heupkwetsuur',\n",
       " 'heusdenaar',\n",
       " 'heusdense',\n",
       " 'hfd.',\n",
       " 'hfd. afd.',\n",
       " 'hfdst.',\n",
       " 'hfst.',\n",
       " 'hft',\n",
       " 'hg',\n",
       " 'hibiscus',\n",
       " 'hidalgo',\n",
       " 'hidjab',\n",
       " 'hielblessure',\n",
       " 'hifi',\n",
       " 'high five',\n",
       " 'hijood',\n",
       " 'hijs',\n",
       " 'hik',\n",
       " 'hilversummer',\n",
       " 'hilversumse',\n",
       " 'himalayakat',\n",
       " 'himbo',\n",
       " 'hindekalf',\n",
       " 'hindoe',\n",
       " 'hindoestaan',\n",
       " 'hindoestaanse',\n",
       " 'hippen',\n",
       " 'hippodroom',\n",
       " 'hipster',\n",
       " 'hipsterhoreca',\n",
       " 'histogram',\n",
       " 'histologie',\n",
       " 'histologisch',\n",
       " 'historicus',\n",
       " 'hit',\n",
       " 'hittebestendig',\n",
       " 'hittebestendigheid',\n",
       " 'hitteplan',\n",
       " 'hittestress',\n",
       " 'hittezoeker',\n",
       " 'hoax',\n",
       " 'hobbelstoel',\n",
       " 'hobbyboer',\n",
       " 'hobbyclub',\n",
       " 'hobbyen',\n",
       " 'hobbykip',\n",
       " 'hobbypaard',\n",
       " 'hobbyvarken',\n",
       " 'hobbyvereniging',\n",
       " 'hobo',\n",
       " 'hobost',\n",
       " 'hockeyblessure',\n",
       " 'hockeybond',\n",
       " 'hockeyclub',\n",
       " 'hockeykwetsuur',\n",
       " 'hockeyploeg',\n",
       " 'hockeyschool',\n",
       " 'hockeyteam',\n",
       " 'hockeytracker',\n",
       " 'hockeyvereniging',\n",
       " 'hoed',\n",
       " 'hoedenmaker',\n",
       " 'hoeder',\n",
       " 'hoedster',\n",
       " 'hoefdier',\n",
       " 'hoeienaar',\n",
       " 'hoeise',\n",
       " 'hoekhuis',\n",
       " 'hoekman',\n",
       " 'hoen',\n",
       " 'hoenderhok',\n",
       " 'hoentje',\n",
       " 'hoepelen',\n",
       " 'hoepelrok',\n",
       " 'hoerastemming',\n",
       " 'hoerenchance',\n",
       " 'hoerenkast',\n",
       " 'hoerenkot',\n",
       " 'hoerenloper',\n",
       " 'hoest',\n",
       " 'hoeve',\n",
       " 'hoevetoerisme',\n",
       " 'hoevetoerist',\n",
       " 'hofdame',\n",
       " 'hofmeester',\n",
       " 'hogereschoolleeftijd',\n",
       " 'hogergenoemd',\n",
       " 'hogeschool',\n",
       " 'hogeschooldocent',\n",
       " 'hogesnelheidslijn',\n",
       " 'hogesnelheidstrein',\n",
       " 'hok',\n",
       " 'hokjesdenken',\n",
       " 'hokjesgeest',\n",
       " 'hokjesmentaliteit',\n",
       " 'holebi',\n",
       " 'holenbeer',\n",
       " 'holenduif',\n",
       " 'holenleeuw',\n",
       " 'holenuil',\n",
       " 'holidate',\n",
       " 'holifeest',\n",
       " 'holikaverbranding',\n",
       " 'holisme',\n",
       " 'hollander',\n",
       " 'hollandse',\n",
       " 'hollen',\n",
       " 'hom',\n",
       " 'homejacker',\n",
       " 'homejacking',\n",
       " 'homeostase',\n",
       " 'homepage',\n",
       " 'homilie',\n",
       " 'hommel',\n",
       " 'hommelsnelweg',\n",
       " 'homo',\n",
       " 'homo homini lupus',\n",
       " 'homocaust',\n",
       " 'homocide',\n",
       " 'homofiel',\n",
       " 'homofobie',\n",
       " 'homofoon',\n",
       " 'homograaf',\n",
       " 'homohaat',\n",
       " 'homohuwelijk',\n",
       " 'homoniem',\n",
       " 'homoseksueel',\n",
       " 'homowijk',\n",
       " 'hond',\n",
       " 'hondenbelasting',\n",
       " 'hondenbezitter',\n",
       " 'hondeneigenaar',\n",
       " 'hondenliefhebber',\n",
       " 'hondenrestaurant',\n",
       " 'hondenrijst',\n",
       " 'hondenschool',\n",
       " 'hondenspeeltuin',\n",
       " 'hondenuitlaatdienst',\n",
       " 'honderdeurobiljet',\n",
       " 'honderdjarige',\n",
       " 'hondje',\n",
       " 'hondurees',\n",
       " 'hondurese',\n",
       " 'hongaar',\n",
       " 'hongaarse',\n",
       " 'hongerklop',\n",
       " 'hongerkunstenaar',\n",
       " 'hongerlijder',\n",
       " 'hongerlijdertje',\n",
       " 'hongerstaker',\n",
       " 'hongerstaking',\n",
       " 'hongerwinter',\n",
       " 'hongkonger',\n",
       " 'hongkongse',\n",
       " 'honing',\n",
       " 'honingbij',\n",
       " 'honingdauw',\n",
       " 'honingdrank',\n",
       " 'honingsnelweg',\n",
       " 'honni soit qui mal y pense',\n",
       " 'hoofdadvocaat-generaal',\n",
       " 'hoofdagent',\n",
       " 'hoofdairbag',\n",
       " 'hoofdakteopleiding',\n",
       " 'hoofdarbeid',\n",
       " 'hoofdbewoner',\n",
       " 'hoofdblessure',\n",
       " 'hoofddirecteur',\n",
       " 'hoofddocent',\n",
       " 'hoofddoek',\n",
       " 'hoofdkantoor',\n",
       " 'hoofdkapitein',\n",
       " 'hoofdkleuterleidster',\n",
       " 'hoofdkwartier',\n",
       " 'hoofdkwetsuur',\n",
       " 'hoofdlozing',\n",
       " 'hoofdluis',\n",
       " 'hoofdmeester',\n",
       " 'hoofdmotor',\n",
       " 'hoofdpijn',\n",
       " 'hoofdpijndossier',\n",
       " 'hoofdpijnportefeuille',\n",
       " 'hoofdtelwoord',\n",
       " 'hoofdvakdocent',\n",
       " 'hoofdwond',\n",
       " 'hoogdag',\n",
       " 'hoogdruk',\n",
       " 'hoogdrukker',\n",
       " 'hoogdrukpers',\n",
       " 'hoogeveense',\n",
       " 'hoogevener',\n",
       " 'hoogheid',\n",
       " 'hooglander',\n",
       " 'hooglandse',\n",
       " 'hoogleraar',\n",
       " 'hoogseizoen',\n",
       " 'hoogstbiedende',\n",
       " 'hoogstudent',\n",
       " 'hoogtepunt',\n",
       " 'hoogtijd',\n",
       " 'hoogtijdag',\n",
       " 'hoogtijperiode',\n",
       " 'hoogvlieger',\n",
       " 'hoogwaardig',\n",
       " 'hoogwaardige',\n",
       " 'hoogwaardigheid',\n",
       " 'hoogwaardigheidsbekleder',\n",
       " 'hoogzomer',\n",
       " 'hooikoortsseizoen',\n",
       " 'hooimijt',\n",
       " 'hooitijd',\n",
       " 'hooiwagen',\n",
       " 'hooizolder',\n",
       " 'hooligan',\n",
       " 'hoop',\n",
       " 'hoorbril',\n",
       " 'hoorn',\n",
       " 'hoornaar',\n",
       " 'hoos',\n",
       " 'hopstaak',\n",
       " 'hora est',\n",
       " 'hordeur',\n",
       " 'horeca',\n",
       " 'horeca-activiteit',\n",
       " 'horeca-etablissement',\n",
       " 'horeca-exploitant',\n",
       " 'horeca-inrichting',\n",
       " 'horeca-uitbater',\n",
       " 'horecabaas',\n",
       " 'horecabedrijf',\n",
       " 'horecabond',\n",
       " 'horecagelegenheid',\n",
       " 'horecagroothandel',\n",
       " 'horecaondernemer',\n",
       " 'horecaonderneming',\n",
       " 'horecapersoneel',\n",
       " 'horecasector',\n",
       " 'horecavergunning',\n",
       " 'horecavoorziening',\n",
       " 'horecawet',\n",
       " 'horecazaak',\n",
       " 'horkenlijn',\n",
       " 'horlogemaker',\n",
       " 'hormonenrund',\n",
       " 'hormoonproducerend',\n",
       " 'hors concours',\n",
       " 'horsmakreel',\n",
       " 'horzel',\n",
       " 'hospitaal',\n",
       " 'hospitawonen',\n",
       " 'hossel',\n",
       " 'hosselaar',\n",
       " 'hosselcultuur',\n",
       " 'hosselen',\n",
       " 'hostel',\n",
       " 'hostess',\n",
       " 'hostie',\n",
       " 'hotdog',\n",
       " 'hotel',\n",
       " 'hotelbediende',\n",
       " 'hoteldebotel',\n",
       " 'hotelgezin',\n",
       " 'hotelrat',\n",
       " 'hotelschool',\n",
       " 'hoteluitbater',\n",
       " 'hotline',\n",
       " 'hotpants',\n",
       " 'hotsaus',\n",
       " 'hotshot',\n",
       " 'houdbaarheidsdatum',\n",
       " 'houseparty',\n",
       " 'housewarmingparty',\n",
       " 'hout',\n",
       " 'houtachtig',\n",
       " 'houtbij',\n",
       " 'houtduif',\n",
       " 'houten klaas',\n",
       " 'houtgravure',\n",
       " 'houthakker',\n",
       " 'houthandelaar',\n",
       " 'houtindustrie',\n",
       " 'houting',\n",
       " 'houtje',\n",
       " 'houtkapvergunning',\n",
       " 'houtskoolschets',\n",
       " 'houtsnede',\n",
       " 'houtsnijder',\n",
       " 'houtsnip',\n",
       " 'houtworm',\n",
       " 'houtwormkever',\n",
       " 'houw',\n",
       " 'houweel',\n",
       " 'houwer',\n",
       " 'houwitser',\n",
       " 'hovaardig',\n",
       " 'hovenier',\n",
       " 'hozen',\n",
       " 'hpv',\n",
       " 'hsl',\n",
       " 'hst',\n",
       " 'hsv',\n",
       " 'hubby',\n",
       " 'hufterindex',\n",
       " 'hufterproof',\n",
       " 'huichelen',\n",
       " 'huidarts',\n",
       " 'huidhonger',\n",
       " 'huidkleurig',\n",
       " 'huidplooi',\n",
       " 'huif',\n",
       " 'huilbaby',\n",
       " 'huis',\n",
       " 'huis-tuin-en-keuken-',\n",
       " 'huisapotheek',\n",
       " 'huisarts',\n",
       " 'huisartsenkring',\n",
       " 'huisartsenopleiding',\n",
       " 'huisartsenpost',\n",
       " 'huisartsenpraktijk',\n",
       " 'huisartsenvereniging',\n",
       " 'huisartsenzorg',\n",
       " 'huisartsgeneeskunde',\n",
       " 'huisartspraktijk',\n",
       " 'huisbaas',\n",
       " 'huisbediende',\n",
       " 'huisbewaarder',\n",
       " 'huisbewoner',\n",
       " 'huisbezitter',\n",
       " 'huisbij',\n",
       " 'huisboktor',\n",
       " 'huisdier',\n",
       " 'huisdierenpolitie',\n",
       " 'huisdokter',\n",
       " 'huiseigenaar',\n",
       " 'huisgenoot',\n",
       " 'huisgezin',\n",
       " 'huishamster',\n",
       " 'huishond',\n",
       " 'huishoudbrood',\n",
       " 'huishoudentoeslag',\n",
       " 'huishoudhulp',\n",
       " 'huishoudonderzoek',\n",
       " 'huishoudschool',\n",
       " 'huishoudwerkloosheid',\n",
       " 'huisjesslak',\n",
       " 'huiskameel',\n",
       " 'huiskamer',\n",
       " 'huiskamerhuwelijk',\n",
       " 'huiskamerrestaurant',\n",
       " 'huiskamervraag',\n",
       " 'huiskat',\n",
       " 'huiskring',\n",
       " 'huismeester',\n",
       " 'huismuis',\n",
       " 'huismus',\n",
       " 'huispak',\n",
       " 'huispersoneel',\n",
       " 'huispoes',\n",
       " 'huisschilder',\n",
       " 'huisspin',\n",
       " 'huisstofmijt',\n",
       " 'huisvarken',\n",
       " 'huisverbod',\n",
       " 'huisvlieg',\n",
       " 'huisvrouw',\n",
       " 'huiswijn',\n",
       " 'huiszwaluw',\n",
       " 'huizenaar',\n",
       " 'huizenbezitter',\n",
       " 'huizenkoper',\n",
       " 'huizense',\n",
       " 'huizenverkoop',\n",
       " 'huizer',\n",
       " 'hulp',\n",
       " 'hulpgeroep',\n",
       " 'hulpgoederen',\n",
       " 'hulphond',\n",
       " 'hulpkonvooi',\n",
       " 'hulpkreet',\n",
       " 'hulpmoeder',\n",
       " 'hulpmotor',\n",
       " 'hulporganisatie',\n",
       " 'hulppakket',\n",
       " 'hulpprogramma',\n",
       " 'hulpproject',\n",
       " 'hulpraket',\n",
       " 'hulpvader',\n",
       " 'hulpverleenster',\n",
       " 'hulpverlener',\n",
       " 'hulpverlening',\n",
       " 'hulpverleningsaanbod',\n",
       " 'hulpverleningsdienst',\n",
       " 'hulpverleningsinstantie',\n",
       " 'hulpverleningsinstelling',\n",
       " 'hulpverleningsorganisatie',\n",
       " 'hulstenaar',\n",
       " 'hulstse',\n",
       " 'humbug',\n",
       " 'hummen',\n",
       " 'humorloos',\n",
       " 'humorloosheid',\n",
       " 'humorvol',\n",
       " 'hunebed',\n",
       " 'hunter',\n",
       " 'huppel',\n",
       " 'huppelen',\n",
       " 'huppeltje',\n",
       " 'huppen',\n",
       " 'husselen',\n",
       " 'hut',\n",
       " 'hutje',\n",
       " 'hutsepot',\n",
       " 'hutspot',\n",
       " 'huur',\n",
       " 'huurder',\n",
       " 'huurdersvereniging',\n",
       " 'huurhuis',\n",
       " 'huurkerstboom',\n",
       " 'huurmoordenaar',\n",
       " 'huurperiode',\n",
       " 'huurtoeslag',\n",
       " 'huurverbreking',\n",
       " 'huurwoning',\n",
       " 'huwelijksbootje',\n",
       " 'huwelijksdag',\n",
       " 'huwelijksjaar',\n",
       " 'huwelijksmakelaar',\n",
       " 'huwelijksmigratie',\n",
       " 'huwelijksmoeilijkheden',\n",
       " 'huwelijksplanner',\n",
       " 'huwelijksverjaardag',\n",
       " 'huzarensalade',\n",
       " 'hybridediesel',\n",
       " 'hybridekameel',\n",
       " 'hybris',\n",
       " 'hydra',\n",
       " 'hydraulica',\n",
       " 'hydrografie',\n",
       " 'hydrologie',\n",
       " 'hyena',\n",
       " 'hyperboek',\n",
       " 'hyperbool',\n",
       " 'hyperopvoeder',\n",
       " 'hyperouder',\n",
       " 'hypertonie',\n",
       " 'hypertoon',\n",
       " 'hypnose',\n",
       " 'hypnotherapeut',\n",
       " 'hypnotiseren',\n",
       " 'hypnotiseur',\n",
       " 'hypochonder',\n",
       " 'hypotheek',\n",
       " 'hypotheekadvies',\n",
       " 'hypotheekadviseur',\n",
       " 'hypotheekgever',\n",
       " 'hypotheekhouder',\n",
       " 'hypotheeknemer',\n",
       " 'hypotheekrente',\n",
       " 'hypotheekverstrekker',\n",
       " 'hypotoon',\n",
       " 'i-generatie',\n",
       " 'i-grec',\n",
       " 'i-profiel',\n",
       " 'i-tijd',\n",
       " 'i.b.v.',\n",
       " 'i.c.m.',\n",
       " 'i.e.',\n",
       " 'i.g.v.',\n",
       " 'i.h.a.',\n",
       " 'i.h.b.',\n",
       " 'i.h.k.v.',\n",
       " 'i.i.g.',\n",
       " 'i.o.',\n",
       " 'i.o.m.',\n",
       " 'i.p.v.',\n",
       " 'i.pl.v.',\n",
       " 'i.s.m.',\n",
       " 'i.t.t.',\n",
       " 'i.v.',\n",
       " 'i.v.m.',\n",
       " 'iaaf',\n",
       " 'iban',\n",
       " 'ibis',\n",
       " 'icbc',\n",
       " 'iconenschilder',\n",
       " 'ict',\n",
       " \"ict'er\",\n",
       " 'ict-afdeling',\n",
       " 'ict-basis',\n",
       " 'ict-bedrijf',\n",
       " 'ict-beleid',\n",
       " 'ict-branche',\n",
       " 'ict-cordinator',\n",
       " 'ict-dienst',\n",
       " 'ict-gebied',\n",
       " 'ict-gebruik',\n",
       " 'ict-gebruiker',\n",
       " 'ict-industrie',\n",
       " 'ict-infrastructuur',\n",
       " 'ict-kennis',\n",
       " 'ict-markt',\n",
       " 'ict-middelen',\n",
       " 'ict-ondersteuning',\n",
       " 'ict-onderzoek',\n",
       " 'ict-ontwikkeling',\n",
       " 'ict-opleiding',\n",
       " 'ict-plan',\n",
       " 'ict-product',\n",
       " 'ict-project',\n",
       " 'ict-protocol',\n",
       " 'ict-revolutie',\n",
       " 'ict-sector',\n",
       " 'ict-systeem',\n",
       " 'ict-technologie',\n",
       " 'ict-toepassing',\n",
       " 'ict-vaardigheid',\n",
       " 'ict-voorziening',\n",
       " 'ict-wereld',\n",
       " 'ideenmakelaar',\n",
       " 'identificatienummer',\n",
       " 'identiteitsdiefstal',\n",
       " 'identiteitsfraude',\n",
       " 'identiteitsgebonden',\n",
       " 'ieperling',\n",
       " 'ieperlinge',\n",
       " 'ieperse',\n",
       " 'ier',\n",
       " 'ierse',\n",
       " 'ietsisme',\n",
       " 'ietsist',\n",
       " \"if you can't beat them, join them\",\n",
       " 'ifab',\n",
       " 'iffr',\n",
       " 'iftar',\n",
       " 'iglo',\n",
       " 'iglotent',\n",
       " 'ignoreren',\n",
       " 'ijdeltuit',\n",
       " 'ijmuidenaar',\n",
       " 'ijmuidense',\n",
       " 'ijs',\n",
       " 'ijsappel',\n",
       " 'ijsbeer',\n",
       " 'ijsberen',\n",
       " 'ijsberg',\n",
       " 'ijsbergsla',\n",
       " 'ijsbox',\n",
       " 'ijsbreker',\n",
       " 'ijsbrommer',\n",
       " 'ijscobromfiets',\n",
       " 'ijscobrommer',\n",
       " 'ijscoman',\n",
       " 'ijsemmeruitdaging',\n",
       " 'ijsgroei',\n",
       " 'ijshut',\n",
       " 'ijsje',\n",
       " 'ijskar',\n",
       " 'ijskast',\n",
       " 'ijskastdiplomatie',\n",
       " 'ijskelder',\n",
       " 'ijskeurmeester',\n",
       " 'ijsklontje',\n",
       " 'ijskonijn',\n",
       " 'ijslam',\n",
       " 'ijslander',\n",
       " 'ijslandse',\n",
       " 'ijsmeester',\n",
       " 'ijsmuts',\n",
       " 'ijspegel',\n",
       " 'ijsschots',\n",
       " 'ijsselmuidenaar',\n",
       " 'ijsselmuidense',\n",
       " 'ijstaart',\n",
       " 'ijsthee',\n",
       " 'ijstijd',\n",
       " 'ijstransplantatie',\n",
       " 'ijsvereniging',\n",
       " 'ijsverkoop',\n",
       " 'ijsverkoper',\n",
       " 'ijswijn',\n",
       " 'ijzel',\n",
       " 'ijzelen',\n",
       " 'ijzerbijter',\n",
       " 'ijzertijd',\n",
       " 'ijzerwinkel',\n",
       " 'ik-cultuur',\n",
       " 'ikea-roman',\n",
       " 'illegitiem',\n",
       " 'illusieloos',\n",
       " 'illusieloosheid',\n",
       " 'illusieloze',\n",
       " 'illusionist',\n",
       " 'image',\n",
       " 'imago',\n",
       " 'imam',\n",
       " 'imker',\n",
       " 'imkersvereniging',\n",
       " 'immigrant',\n",
       " 'immigreren',\n",
       " 'immobilinkantoor',\n",
       " 'immunologie',\n",
       " 'immunologisch',\n",
       " 'immunoloog',\n",
       " 'imperator',\n",
       " 'impermeabel',\n",
       " 'importbruid',\n",
       " 'importeur',\n",
       " 'impressionist',\n",
       " 'imprint',\n",
       " 'impromptu',\n",
       " 'improvisatietalent',\n",
       " 'improvisator',\n",
       " 'impulsklas',\n",
       " 'in absentia',\n",
       " 'in allen dele',\n",
       " 'in allen gevalle',\n",
       " 'in dezer voege',\n",
       " 'in dier voege',\n",
       " 'in dubio',\n",
       " 'in dubio pro reo',\n",
       " 'in extenso',\n",
       " 'in genen dele',\n",
       " 'in goeden doen',\n",
       " 'in groten getale',\n",
       " 'in groteren getale',\n",
       " 'in optima forma',\n",
       " 'in plano',\n",
       " 'in se',\n",
       " 'in triplo',\n",
       " 'in usum delphini',\n",
       " 'in voce',\n",
       " 'in-vitrofertilisatie',\n",
       " 'inauguratie',\n",
       " 'inaugureel',\n",
       " 'inbox',\n",
       " 'inbraakpreventie',\n",
       " 'inbreker',\n",
       " 'inbrengwinkel',\n",
       " 'inbreukmaker',\n",
       " 'inburgeraar',\n",
       " 'inburgeringseis',\n",
       " 'inburgeringsexamen',\n",
       " 'inburgeringstoets',\n",
       " 'inburgeringstraject',\n",
       " 'incestvader',\n",
       " 'incheckpaal',\n",
       " 'incisie',\n",
       " 'incl.',\n",
       " 'incongruent',\n",
       " 'incrowd',\n",
       " 'incubatieperiode',\n",
       " 'incubatietijd',\n",
       " 'indertijd',\n",
       " 'indiaan',\n",
       " 'indiaantje',\n",
       " 'indianenpaard',\n",
       " 'indigokind',\n",
       " 'indische',\n",
       " 'indir',\n",
       " 'indoctrineren',\n",
       " 'indolent',\n",
       " 'indonesische',\n",
       " 'indonesir',\n",
       " 'indooratletiek',\n",
       " 'indoorkampioen',\n",
       " 'indoorkampioenschap',\n",
       " 'indoorseizoen',\n",
       " 'indoorwedstrijd',\n",
       " 'industrie',\n",
       " 'industrieapotheker',\n",
       " 'industriebank',\n",
       " 'industriebeleid',\n",
       " 'industriebond',\n",
       " 'industriebouw',\n",
       " 'industriecentrum',\n",
       " 'industriecomplex',\n",
       " 'industrieconcern',\n",
       " 'industrieconglomeraat',\n",
       " 'industriediamant',\n",
       " 'industrieel',\n",
       " 'industriefonds',\n",
       " 'industriegebied',\n",
       " 'industriegrond',\n",
       " 'industriehaven',\n",
       " 'industriekring',\n",
       " 'industrieland',\n",
       " 'industriepark',\n",
       " 'industriepolitiek',\n",
       " 'industrieproduct',\n",
       " 'industrieschap',\n",
       " 'industriesector',\n",
       " 'industriestaat',\n",
       " 'industriestad',\n",
       " 'industrietak',\n",
       " 'industrieterrein',\n",
       " 'industriewater',\n",
       " 'industriezone',\n",
       " 'infant',\n",
       " 'infante',\n",
       " 'infectie',\n",
       " 'infinitief',\n",
       " 'infirmerie',\n",
       " 'inflatiebestendig',\n",
       " 'inflatiebestendigheid',\n",
       " 'inflatiespiraal',\n",
       " 'info',\n",
       " 'infoavond',\n",
       " 'infobesitas',\n",
       " 'infodag',\n",
       " 'infographic',\n",
       " 'informaticadocent',\n",
       " 'informatie',\n",
       " 'informatieambtenaar',\n",
       " 'informatieavond',\n",
       " 'informatiedag',\n",
       " 'informatiedrager',\n",
       " 'informatieochtend',\n",
       " 'informatieprofessional',\n",
       " 'informatietijdperk',\n",
       " 'infotainment',\n",
       " 'infraroodkijker',\n",
       " 'infraroodzoeker',\n",
       " 'infrastructuur',\n",
       " 'infusie',\n",
       " 'ing.',\n",
       " 'ingangsdatum',\n",
       " 'ingenieur',\n",
       " 'ingipipa',\n",
       " 'ingroeiperiode',\n",
       " 'inhaalassistent',\n",
       " 'inhaalwedstrijd',\n",
       " 'inheemse',\n",
       " 'inhoudswoord',\n",
       " 'initiaalwoord',\n",
       " 'initiatiefneemster',\n",
       " 'initiatiefnemer',\n",
       " 'initiator',\n",
       " 'injectiemotor',\n",
       " 'inklaarder',\n",
       " 'inkloppen',\n",
       " 'inkomenstoets',\n",
       " 'inkoopkracht',\n",
       " 'inkttekening',\n",
       " 'inktvis',\n",
       " 'inleenkracht',\n",
       " 'inleesvoorstelling',\n",
       " 'inleider',\n",
       " 'inlelijk',\n",
       " 'inleunwoning',\n",
       " 'inlognaam',\n",
       " 'inloopavond',\n",
       " 'inloopochtend',\n",
       " 'innovatiegericht',\n",
       " 'inrijperiode',\n",
       " 'inruilboom',\n",
       " 'ins blaue hinein',\n",
       " 'inschrijftermijn',\n",
       " 'inschrijvingsperiode',\n",
       " 'inschrijvingstermijn',\n",
       " 'insect',\n",
       " 'insecteneter',\n",
       " 'insectensnack',\n",
       " 'insectivoor',\n",
       " 'insemineren',\n",
       " 'inslaper',\n",
       " 'inspanningsverplichting',\n",
       " 'inspecteur',\n",
       " 'inspiratiedag',\n",
       " 'inspirator',\n",
       " 'inspiratrice',\n",
       " 'inspraakavond',\n",
       " 'inspraakmogelijkheid',\n",
       " 'inspraakochtend',\n",
       " 'inspraaktermijn',\n",
       " 'instagrammen',\n",
       " 'installatiesoftware',\n",
       " 'instapmoment',\n",
       " 'instapper',\n",
       " 'instructeur',\n",
       " 'instructiebad',\n",
       " 'instructrice',\n",
       " 'instrumentmaker',\n",
       " 'integrator',\n",
       " 'integritis',\n",
       " 'intekenaar',\n",
       " 'interbankentoernooi',\n",
       " 'intercity',\n",
       " 'intercitytrein',\n",
       " 'interdistrictentoernooi',\n",
       " 'interieurarchitect',\n",
       " 'interim-directeur',\n",
       " 'interim-periode',\n",
       " 'interim-voorzitter',\n",
       " 'interimaris',\n",
       " 'interland',\n",
       " 'interlandverplichting',\n",
       " 'interlandwedstrijd',\n",
       " 'internaut',\n",
       " 'internet',\n",
       " 'internetaanbieder',\n",
       " 'internetadres',\n",
       " 'internetbankieren',\n",
       " 'internetbellen',\n",
       " 'internetbubbel',\n",
       " 'internetcaf',\n",
       " 'internetcriminaliteit',\n",
       " 'internetcrimineel',\n",
       " 'internetdemocratie',\n",
       " 'internetdetective',\n",
       " 'internetdiploma',\n",
       " 'internetevangelist',\n",
       " 'internetfiets',\n",
       " 'internetgebruiker',\n",
       " 'internetgedrag',\n",
       " 'internetgeneratie',\n",
       " 'internetgoeroe',\n",
       " 'internetinspecteur',\n",
       " 'internetjunk',\n",
       " 'internetkrant',\n",
       " 'internetlek',\n",
       " 'internetondernemer',\n",
       " 'internetpagina',\n",
       " 'internetparkeren',\n",
       " 'internetradio',\n",
       " 'internetrechercheur',\n",
       " 'internetrechtbank',\n",
       " 'internetspaarder',\n",
       " 'internetspaarrekening',\n",
       " 'internetsparen',\n",
       " 'internetstalking',\n",
       " 'internetsurveillant',\n",
       " 'internettijd',\n",
       " 'internettijdperk',\n",
       " 'internettrol',\n",
       " 'internetverkoop',\n",
       " 'internetwinkel',\n",
       " 'internetwinkelier',\n",
       " 'internetzeepbel',\n",
       " 'internist',\n",
       " 'interpellatievoorstel',\n",
       " 'interpretatiefout',\n",
       " 'interruptie',\n",
       " 'interseks',\n",
       " 'intersekse',\n",
       " 'interseksueel',\n",
       " 'interview',\n",
       " 'interviewer',\n",
       " 'interviewster',\n",
       " 'intimidatiecultuur',\n",
       " 'intoneren',\n",
       " 'intranet',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left context</th>\n",
       "      <th>lemma 0</th>\n",
       "      <th>universal_dependency 0</th>\n",
       "      <th>word 0</th>\n",
       "      <th>right context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.iiii. ghecorne gulde broeders die de</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke</td>\n",
       "      <td>oudenden sin. si moghen elc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viere ghecorne guldebroeders die de</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke</td>\n",
       "      <td>houden si moghen elc haren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.iiii. ghecorne guldebroeders die de</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boke</td>\n",
       "      <td>ouden si moghen elc haren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>secundi willelmus de lapide willelmus</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>Jn elst. arnulphus de keelne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heren M CC LXXX, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen csens es mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>van poschen, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen pagt es mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>van poschen, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen pagt es mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>van poschen, due wart dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buc</td>\n",
       "      <td>begonnen. Dese pegte es mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>van poschen, due wart dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buc</td>\n",
       "      <td>begonnen. Dit blift den bruderen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>van poschen, due wart dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buc</td>\n",
       "      <td>begonnen. Dit sin degene die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>van posschen, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen csens sin we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>van posschen, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen csens sin we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Heren M CC LXXX, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen pagt sin de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Heren M CC LXXX, due wart det</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>begonnen. Desen pagt sin de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dese</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bouc</td>\n",
       "      <td>was ghemaect int iaer ons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ribode anduorden soude vp den</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boech</td>\n",
       "      <td>.. xxxv s Jtem pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>de leie. Woutre van der</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bouke.</td>\n",
       "      <td>.xviij. d. Heinric mulkins. huus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wolfaert met den alse, jan</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bouc</td>\n",
       "      <td>, jan clais zone ende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dit es de</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bouc</td>\n",
       "      <td>vander renten ende vander heruachtecheden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sente bamesse Woutre van der</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bouke</td>\n",
       "      <td>.xviij. d. sente bamesse ande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tienne scellinghe arfelijcs cheins. ten</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boecken</td>\n",
       "      <td>behoef sclousters van vorst. teghelde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gheset ten ambachte der voer ghenoemder</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boecke</td>\n",
       "      <td>ende alse selc paiment alse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.i. broet. Lammin van der</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke</td>\n",
       "      <td>.i. broet arnout gardiser over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>e[ne waerf] [was] Dar wa[s] [een]</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bo[ec]</td>\n",
       "      <td>en[de] [hic] [las] Dat een</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>niemene vand soe gheme[ne] die</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>hi segt ons ouer waer Dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sijn leuen ewelike amen Dese</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>es nuttelec sere Diene leest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sullen wesen Dar men desen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>sal lesen Salujt gheluc ende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sal wesen Dar men desen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>sal lesen Ende sinen sijn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wel dran leegt Andat dese</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>hier seeght Ende hijt wille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>biechten gan Hier ent de</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>van der biechten Hier beghint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>gaue van genaden ouer al alst</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>hier na getughen sal. [10] Wie dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>beginne dede van den irsten</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke</td>\n",
       "      <td>ghewach daer se een edele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Nu wil ic dit ander</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>hier jnden daer men mach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>mi eer ic dat derde</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>beginne om te vergederen te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[1] Hier begint dat derde</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>van der geder sinte lutgarden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>na dien dat steet ind</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>der gesteleker minnen Haer ureghd eens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>nv op dees stonde vten</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke</td>\n",
       "      <td>dies leuens dar ghi mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>spreken af Hier beslute ict</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>themale.. O edele leserse ende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Want als man hort en</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bk</td>\n",
       "      <td>lesen walsg is of dtsg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>tegenwordig en bin dat dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>mi tegenwordig make mit der</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>sal v. togen.  dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>heuet varwe inde worde. inde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>dat ic mi pine dsgedain</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>te scriuene inde te makene.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>seluer nmtig gemakt mit dsen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buke</td>\n",
       "      <td>te digtene inde te scriuene.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>dn d leringe. dat dt</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>lrt. Jnde seget. h sal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>beginsel. dar mbe sal man dit</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>dicke horen. Dar mbe segt d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>v. te dne dat dt</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>seget. beide mit munde inde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Als du lesis in en</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buk</td>\n",
       "      <td>pinse dat dar gen valgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>coitus hiwelech coeternus eweleke codex</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buch</td>\n",
       "      <td>coercere bedwingen coherere te samene hangen coequare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>te stornisse exicialis dolec exodus ein</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boech</td>\n",
       "      <td>exorcizare beswerren exorcismus besweringe exorcista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>ligteleke leuita diaken leuiticus ein</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buec</td>\n",
       "      <td>leua wenster hant leuus slinc leuca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>offren, smaken Libatio offrande liber</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buek</td>\n",
       "      <td>libellus bucsken librarjus schriuere liberi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>te makene, ende makdender af grote</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bueke</td>\n",
       "      <td>ende grote [ghescreften] die vele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>ende men brachte hem enen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>dat was de prophecie Ysaie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>propheten. Doe ontploec Ihesus din</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>ende quam ten irsten male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>hadde, so loec hi den</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boec</td>\n",
       "      <td>weder toe ende gavene op</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>want hi screef in sinen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeken</td>\n",
       "      <td>van mi. Ende ochte ghi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>dat mogdi pruuen ute Moyses</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>buken</td>\n",
       "      <td>bi din warde dat God</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>gescreuen en sijn [in] desen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke,</td>\n",
       "      <td>mar dese sijn gescreuen, om dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>gescreuen en sijn in desen</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>boeke;</td>\n",
       "      <td>want soude ment al bescriuen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>ende dat hi leerde, die</td>\n",
       "      <td>BOEK</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bueke</td>\n",
       "      <td>die men daer af maken soude,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 left context lemma 0 universal_dependency 0  \\\n",
       "0      .iiii. ghecorne gulde broeders die de     BOEK                   NOUN   \n",
       "1        viere ghecorne guldebroeders die de     BOEK                   NOUN   \n",
       "2       .iiii. ghecorne guldebroeders die de     BOEK                   NOUN   \n",
       "3      secundi willelmus de lapide willelmus     BOEK                   NOUN   \n",
       "4              Heren M CC LXXX, due wart det     BOEK                   NOUN   \n",
       "5                  van poschen, due wart det     BOEK                   NOUN   \n",
       "6                  van poschen, due wart det     BOEK                   NOUN   \n",
       "7                  van poschen, due wart dit     BOEK                   NOUN   \n",
       "8                  van poschen, due wart dit     BOEK                   NOUN   \n",
       "9                  van poschen, due wart dit     BOEK                   NOUN   \n",
       "10                van posschen, due wart det     BOEK                   NOUN   \n",
       "11                van posschen, due wart det     BOEK                   NOUN   \n",
       "12             Heren M CC LXXX, due wart det     BOEK                   NOUN   \n",
       "13             Heren M CC LXXX, due wart det     BOEK                   NOUN   \n",
       "14                                      Dese     BOEK                   NOUN   \n",
       "15             ribode anduorden soude vp den     BOEK                   NOUN   \n",
       "16                   de leie. Woutre van der     BOEK                   NOUN   \n",
       "17                wolfaert met den alse, jan     BOEK                   NOUN   \n",
       "18                                 Dit es de     BOEK                   NOUN   \n",
       "19              sente bamesse Woutre van der     BOEK                   NOUN   \n",
       "20   tienne scellinghe arfelijcs cheins. ten     BOEK                   NOUN   \n",
       "21   gheset ten ambachte der voer ghenoemder     BOEK                   NOUN   \n",
       "22                 .i. broet. Lammin van der     BOEK                   NOUN   \n",
       "23         e[ne waerf] [was] Dar wa[s] [een]     BOEK                   NOUN   \n",
       "24            niemene vand soe gheme[ne] die     BOEK                   NOUN   \n",
       "25              sijn leuen ewelike amen Dese     BOEK                   NOUN   \n",
       "26                sullen wesen Dar men desen     BOEK                   NOUN   \n",
       "27                   sal wesen Dar men desen     BOEK                   NOUN   \n",
       "28                 wel dran leegt Andat dese     BOEK                   NOUN   \n",
       "29                  biechten gan Hier ent de     BOEK                   NOUN   \n",
       "..                                        ...     ...                    ...   \n",
       "234            gaue van genaden ouer al alst     BOEK                   NOUN   \n",
       "235              beginne dede van den irsten     BOEK                   NOUN   \n",
       "236                      Nu wil ic dit ander     BOEK                   NOUN   \n",
       "237                      mi eer ic dat derde     BOEK                   NOUN   \n",
       "238                [1] Hier begint dat derde     BOEK                   NOUN   \n",
       "239                    na dien dat steet ind     BOEK                   NOUN   \n",
       "240                   nv op dees stonde vten     BOEK                   NOUN   \n",
       "241              spreken af Hier beslute ict     BOEK                   NOUN   \n",
       "242                     Want als man hort en     BOEK                   NOUN   \n",
       "243               tegenwordig en bin dat dit     BOEK                   NOUN   \n",
       "244                      sal v. togen.  dit     BOEK                   NOUN   \n",
       "245                 dat ic mi pine dsgedain     BOEK                   NOUN   \n",
       "246          seluer nmtig gemakt mit dsen     BOEK                   NOUN   \n",
       "247                  dn d leringe. dat dt     BOEK                   NOUN   \n",
       "248           beginsel. dar mbe sal man dit     BOEK                   NOUN   \n",
       "249                       v. te dne dat dt     BOEK                   NOUN   \n",
       "250                       Als du lesis in en     BOEK                   NOUN   \n",
       "251  coitus hiwelech coeternus eweleke codex     BOEK                   NOUN   \n",
       "252  te stornisse exicialis dolec exodus ein     BOEK                   NOUN   \n",
       "253    ligteleke leuita diaken leuiticus ein     BOEK                   NOUN   \n",
       "254    offren, smaken Libatio offrande liber     BOEK                   NOUN   \n",
       "255       te makene, ende makdender af grote     BOEK                   NOUN   \n",
       "256                ende men brachte hem enen     BOEK                   NOUN   \n",
       "257       propheten. Doe ontploec Ihesus din     BOEK                   NOUN   \n",
       "258                    hadde, so loec hi den     BOEK                   NOUN   \n",
       "259                  want hi screef in sinen     BOEK                   NOUN   \n",
       "260              dat mogdi pruuen ute Moyses     BOEK                   NOUN   \n",
       "261             gescreuen en sijn [in] desen     BOEK                   NOUN   \n",
       "262               gescreuen en sijn in desen     BOEK                   NOUN   \n",
       "263                  ende dat hi leerde, die     BOEK                   NOUN   \n",
       "\n",
       "      word 0                                           right context  \n",
       "0      boeke                             oudenden sin. si moghen elc  \n",
       "1      boeke                              houden si moghen elc haren  \n",
       "2       boke                               ouden si moghen elc haren  \n",
       "3       boec                            Jn elst. arnulphus de keelne  \n",
       "4       buec                            begonnen. Desen csens es mer  \n",
       "5       buec                             begonnen. Desen pagt es mer  \n",
       "6       buec                             begonnen. Desen pagt es mer  \n",
       "7        buc                             begonnen. Dese pegte es mer  \n",
       "8        buc                        begonnen. Dit blift den bruderen  \n",
       "9        buc                            begonnen. Dit sin degene die  \n",
       "10      buec                            begonnen. Desen csens sin we  \n",
       "11      buec                            begonnen. Desen csens sin we  \n",
       "12      buec                             begonnen. Desen pagt sin de  \n",
       "13      buec                             begonnen. Desen pagt sin de  \n",
       "14      bouc                               was ghemaect int iaer ons  \n",
       "15     boech                                      .. xxxv s Jtem pro  \n",
       "16    bouke.                        .xviij. d. Heinric mulkins. huus  \n",
       "17      bouc                                   , jan clais zone ende  \n",
       "18      bouc               vander renten ende vander heruachtecheden  \n",
       "19     bouke                           .xviij. d. sente bamesse ande  \n",
       "20   boecken                   behoef sclousters van vorst. teghelde  \n",
       "21    boecke                             ende alse selc paiment alse  \n",
       "22     boeke                          .i. broet arnout gardiser over  \n",
       "23    bo[ec]                              en[de] [hic] [las] Dat een  \n",
       "24      boec                               hi segt ons ouer waer Dat  \n",
       "25      boec                            es nuttelec sere Diene leest  \n",
       "26      boec                            sal lesen Salujt gheluc ende  \n",
       "27      boec                               sal lesen Ende sinen sijn  \n",
       "28      boec                             hier seeght Ende hijt wille  \n",
       "29      boec                           van der biechten Hier beghint  \n",
       "..       ...                                                     ...  \n",
       "234     boec                      hier na getughen sal. [10] Wie dat  \n",
       "235    boeke                               ghewach daer se een edele  \n",
       "236     boec                                hier jnden daer men mach  \n",
       "237     boec                             beginne om te vergederen te  \n",
       "238     boec                          van der geder sinte lutgarden  \n",
       "239     boec                  der gesteleker minnen Haer ureghd eens  \n",
       "240    boeke                                  dies leuens dar ghi mi  \n",
       "241     boec                          themale.. O edele leserse ende  \n",
       "242      bk                                lesen walsg is of dtsg.  \n",
       "243      buk                             mi tegenwordig make mit der  \n",
       "244      buk                            heuet varwe inde worde. inde  \n",
       "245      buk                             te scriuene inde te makene.  \n",
       "246     buke                            te digtene inde te scriuene.  \n",
       "247      buk                                lrt. Jnde seget. h sal  \n",
       "248      buk                           dicke horen. Dar mbe segt d  \n",
       "249      buk                             seget. beide mit munde inde  \n",
       "250      buk                                 pinse dat dar gen valgs  \n",
       "251     buch   coercere bedwingen coherere te samene hangen coequare  \n",
       "252    boech    exorcizare beswerren exorcismus besweringe exorcista  \n",
       "253     buec                     leua wenster hant leuus slinc leuca  \n",
       "254     buek             libellus bucsken librarjus schriuere liberi  \n",
       "255    bueke                       ende grote [ghescreften] die vele  \n",
       "256     boec                              dat was de prophecie Ysaie  \n",
       "257     boec                               ende quam ten irsten male  \n",
       "258     boec                                weder toe ende gavene op  \n",
       "259   boeken                                  van mi. Ende ochte ghi  \n",
       "260    buken                                    bi din warde dat God  \n",
       "261   boeke,                         mar dese sijn gescreuen, om dat  \n",
       "262   boeke;                            want soude ment al bescriuen  \n",
       "263    bueke                            die men daer af maken soude,  \n",
       "\n",
       "[264 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db9b1c31a524ae3b33c8cdcb73f93cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Sla uw resultaten op:'), Text(value='mijn_resultaten.csv'), Button(button_style='w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query lexicon to give list of all words\n",
    "lexicon=\"anw\"\n",
    "df_lexicon = search_lexicon_alllemmata(lexicon)\n",
    "## TODO: Why do double words appear?\n",
    "lexicon_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "display(lexicon_set)\n",
    "\n",
    "#TODO: we search one lemma for now\n",
    "q = corpus_query_lemma(\"boek\")\n",
    "df_corpus = search_corpus(q,\"gysseling\")\n",
    "display_df(df_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "## Case study (sequential) 4: Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T13:34:13.249Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_to_search=\"opensonar\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Get occurences of attributive adjectives not ending with -e')\n",
    "df_corpus = search_corpus(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"ADJ\"][pos=\"NOUN\"]', corpus=corpus_to_search)\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "query=lexicon_query('^g(.+)[^e]$', 'ADJ', lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "final_e_condition = df_filter(df_lexicon[\"wordform\"], 'e$')\n",
    "df_lexicon_form_e = df_lexicon[ final_e_condition ]\n",
    "display(df_lexicon_form_e)\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('List of attributive adjectives not ending with -e even though they should have a final -e:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "no_final_e_condition = df_filter(df_corpus[\"word 1\"], e_forms)\n",
    "result_df = df_corpus[ no_final_e_condition ]\n",
    "display( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study (sequential) 5: (morphosyntactic lexicon and possibly unannotated corpus) Look up inflected forms and spelling variants for a given lemma in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.744390Z",
     "start_time": "2019-02-04T17:27:48.101Z"
    }
   },
   "outputs": [],
   "source": [
    "lexicon_to_search=\"molex\"\n",
    "corpus_to_search=\"chn\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "query=lexicon_query(lemma_to_look_for, None, lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "df_corpus = search_corpus(query, corpus=corpus_to_search)\n",
    "display(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 6: Build frequency table of some corpus, based on lemma list of a given lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.745116Z",
     "start_time": "2019-02-04T17:27:48.107Z"
    }
   },
   "outputs": [],
   "source": [
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list1.sort_values(ascending=False,by=['raw_freq']).head(25)\n",
    "df_top25_ascending = df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display( df_top25_descending )\n",
    "display_df( df_top25['raw_freq'], labels='chart df1', mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list2.sort_values(ascending=False,by=['raw_freq']).head(25)\n",
    "df_top25_ascending = df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display( df_top25_descending )\n",
    "display_df( df_top25['raw_freq'], labels='chart df2', mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "# sort and display\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "\n",
    "display(df_rankdiffs.sort_values(by=['rank_diff']).head(25))\n",
    "\n",
    "df_top25_descending = df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_descending['rank_diff'], labels='chart large diff', mode='chart' )\n",
    "\n",
    "df_top25_ascending = df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_ascending['rank_diff'], labels='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 7: search in a corpus for wordforms of a lemma, which are not included in this lemma's paramadigm in a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.745942Z",
     "start_time": "2019-02-04T17:27:48.110Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search=\"opensonar\"\n",
    "\n",
    "df = get_missing_wordforms(base_lexicon, \"VERB\", corpus_to_search)\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "#df = load_dataframe(\"missing_wordforms.csv\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 8: Train a tagger with data from an annotated corpus, and do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.746715Z",
     "start_time": "2019-02-04T17:27:48.113Z"
    }
   },
   "outputs": [],
   "source": [
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# we have a given word, let's say: \"loop\"\n",
    "some_word = \"loop\"\n",
    "\n",
    "# get the paradigm of the lemma our word is a part of\n",
    "query = lexicon_query(some_word, pos=None, lexicon=base_lexicon)\n",
    "df_paradigm = search_lexicon(query, base_lexicon)\n",
    "display(df_paradigm)\n",
    "\n",
    "# gather some pattern including our word, out of an annotated corpus\n",
    "# here: DET + ADJ + 'loop'\n",
    "corpus_query = corpus_query_wordform(some_word)\n",
    "corpus_opts = corpus_options( detailed_context=True )\n",
    "\n",
    "df_corpus1 = search_corpus(corpus_query, corpus=corpus_to_search1, corpus_opts)\n",
    "display(df_corpus1)\n",
    "df_corpus2 = search_corpus(corpus_query, corpus=corpus_to_search2, corpus_opts)\n",
    "display(df_corpus2)\n",
    "\n",
    "\n",
    "df_all = concat_df([df_corpus1, df_corpus2], [corpus_to_search1, corpus_to_search2])\n",
    "display(df_all)\n",
    "\n",
    "# get a tagger trained with our corpus data\n",
    "tagger = get_tagger(df_all)\n",
    "\n",
    "# Use the trained tagger to tag unknown sentences\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "sentence = 'Mijn buurman kijkt door de loop van zijn geweer'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 9: Search in corpus and filter on metadata\n",
    "First, we request all available metadata fields of the corpus. Then, we issue a search query, and request all metadata fields for the result. Finally, we filter on metadata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T12:26:02.818127Z",
     "start_time": "2019-02-12T12:25:44.726260Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_name=\"zeebrieven\"\n",
    "query=r'[lemma=\"dat\"]'\n",
    "# Request all metadata fields from corpus\n",
    "fields = get_available_metadata(corpus_name)\n",
    "# Perform query and ask all metadata\n",
    "corpus_opts = corpus_options(extra_fields_doc=fields[\"document\"])\n",
    "df_corpus = search_corpus(query, corpus_name, corpus_opts)\n",
    "\n",
    "# Filter on year: > 1700\n",
    "df_filter_year = df_corpus[df_corpus[\"witnessYear_from\"].astype('int32') > 1700] \n",
    "display_df(df_filter_year, title=\"After 1700\")\n",
    "\n",
    "# Filter on sender birth place Amsterdam\n",
    "condition = filter_condition(df_corpus, column=\"afz_geb_plaats\", method=\"contains_regex\", regex_or_set=\"Amsterdam\")\n",
    "df_filter_place = df_corpus[ condition ]\n",
    "display_df(df_filter_place, title=\"Sender born in Amsterdam\")\n",
    "\n",
    "\n",
    "# Group by birth place\n",
    "df = property_freq(df_corpus,\"afz_loc_plaats\")\n",
    "display_df(df, title=\"Most frequent sender locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 10: Visualizing h-dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-12T13:38:44.098Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_to_search=\"chn\"\n",
    "\n",
    "fields = get_available_metadata(corpus_to_search)\n",
    "#print(fields)\n",
    "corpus_opts = corpus_options(extra_fields_doc=fields[\"document\"])\n",
    "\n",
    "df_corpus1 = search_corpus(r'[lemma=\"h[aeo].*\" & word=\"[aeo].*\"]', corpus_to_search, corpus_opts)\n",
    "df_corpus2 = search_corpus(r'[lemma=\"h[aeo].*\" & word=\"h[aeo].*\"]', corpus_to_search, corpus_opts)\n",
    "\n",
    "display_df( df_corpus1)\n",
    "display_df( df_corpus2)\n",
    "\n",
    "display_df( df_corpus1.groupby([\"Region\", \"Date\"]), labels=\"h-dropping\", mode='chart')\n",
    "display_df( df_corpus2.groupby([\"Region\", \"Date\"]), labels=\"normal\", mode='chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
